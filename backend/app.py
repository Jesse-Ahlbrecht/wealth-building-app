from flask import Flask, jsonify, request, g
from flask_cors import CORS
import csv
from datetime import datetime, timezone, timedelta
from collections import defaultdict
import os
import re
import glob
import json
import base64
from functools import wraps
from typing import Any, Dict, Optional
from PyPDF2 import PdfReader
from dotenv import load_dotenv

# Load environment variables FIRST, before importing modules that need them
load_dotenv()

from encryption import get_encryption_service, encrypt_sensitive_data, decrypt_sensitive_data, EncryptedData
from auth import get_session_manager
from database import get_wealth_database
from user_management import get_user_manager
from prediction_service import RecurringPatternDetector, get_dismissed_predictions

app = Flask(__name__)
CORS(app)

# Initialize session manager and database
session_manager = get_session_manager()
wealth_db = get_wealth_database()
user_manager = get_user_manager(wealth_db.db)

DOCUMENT_TYPES = [
    {
        'key': 'bank_statement_dkb',
        'label': 'DKB Bank Statement',
        'category': 'bank',
        'description': 'CSV exports from Deutsche Kreditbank (DKB) Girokonto or Tagesgeld accounts.',
        'extensions': ['.csv'],
        'parser': 'parse_dkb',
        'sample_data_path': 'data/bank_statements/dkb'
    },
    {
        'key': 'bank_statement_yuh',
        'label': 'YUH Activity Export',
        'category': 'bank',
        'description': 'Activity exports from the YUH app stored as CSV files.',
        'extensions': ['.csv'],
        'parser': 'parse_yuh',
        'sample_data_path': 'data/bank_statements/yuh'
    },
    {
        'key': 'broker_viac_pdf',
        'label': 'VIAC Trade Confirmation',
        'category': 'broker',
        'description': 'Trade confirmation PDFs generated by VIAC.',
        'extensions': ['.pdf'],
        'parser': 'parse_viac',
        'sample_data_path': 'data/depot_transactions/viac'
    },
    {
        'key': 'broker_ing_diba_csv',
        'label': 'ING DiBa Depot Overview',
        'category': 'broker',
        'description': 'Depot overview CSV exports from ING DiBa.',
        'extensions': ['.csv'],
        'parser': 'parse_ing_diba',
        'sample_data_path': 'data/depot_transactions/ing_diba'
    },
    {
        'key': 'loan_kfw_pdf',
        'label': 'KfW Loan Statement',
        'category': 'loan',
        'description': 'KfW student loan account statements as PDF files.',
        'extensions': ['.pdf'],
        'parser': 'parse_kfw',
        'sample_data_path': 'data/credits/kfw'
    }
]

DOCUMENT_TYPE_LOOKUP = {doc['key']: doc for doc in DOCUMENT_TYPES}


def _get_authenticated_user_id() -> Optional[int]:
    """Extract the authenticated user's integer ID from session claims."""
    if not getattr(g, 'session_claims', None):
        return None

    sub = g.session_claims.get('sub')
    if sub is None:
        return None

    try:
        return int(sub)
    except (TypeError, ValueError):
        return None


def _serialize_document_record(record: Dict[str, Any]) -> Dict[str, Any]:
    """
    Convert a raw attachment record into the API response shape.
    """
    uploaded_at = record.get('uploaded_at')
    if hasattr(uploaded_at, 'isoformat'):
        uploaded_at = uploaded_at.isoformat()

    metadata = record.get('metadata') or {}
    file_info = {}
    client_metadata = {}
    document_metadata = {}

    if isinstance(metadata, dict):
        file_info = metadata.get('file_info') or {}
        client_metadata = metadata.get('client_encryption', {}).get('fileMetadata') or {}
        document_metadata = (
            metadata.get('document_metadata')
            or metadata.get('user_metadata')
            or metadata.get('fileMetadata')
            or {}
        )

    return {
        'id': record.get('id'),
        'documentType': record.get('file_type'),
        'originalName': record.get('original_name'),
        'fileSize': record.get('file_size'),
        'mimeType': record.get('mime_type'),
        'uploadedAt': uploaded_at,
        'uploadedBy': record.get('uploaded_by'),
        'checksum': record.get('checksum'),
        'fileInfo': file_info or {},
        'clientMetadata': client_metadata or {},
        'documentMetadata': document_metadata or {}
    }

def authenticate_request(f):
    """
    Decorator to authenticate API requests and sign responses
    """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        # Get session token from header
        session_token = request.headers.get('Authorization')
        if session_token and session_token.startswith('Bearer '):
            session_token = session_token[7:]  # Remove 'Bearer ' prefix

        # Validate session
        session_claims = None
        if session_token:
            session_claims = session_manager.validate_session(session_token)

        # Store session info in Flask g object for use in endpoint
        g.session_claims = session_claims
        g.session_token = session_token

        # Call the actual endpoint
        response_data = f(*args, **kwargs)

        # Handle tuple responses (data, status_code)
        status_code = 200
        if isinstance(response_data, tuple):
            response_data, status_code = response_data

        # If it's already a Response object (like from jsonify), extract the JSON data
        if hasattr(response_data, 'get_json'):
            # It's a Response object, extract the JSON data
            json_data = response_data.get_json()
            if json_data is None:
                # If it's not JSON data, return as-is (error responses, etc.)
                return response_data
            response_data = json_data

        # Sign the response
        signed_response = session_manager.create_signed_api_response(
            response_data if isinstance(response_data, dict) else {'data': response_data},
            session_token
        )

        return jsonify(signed_response), status_code

    return decorated_function

def require_auth(f):
    """
    Decorator that requires valid authentication
    """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not g.session_claims:
            return jsonify({'error': 'Authentication required'}), 401
        return f(*args, **kwargs)
    return decorated_function

# Note: Categories are now stored in the database (categories table)
# Transaction category updates are stored in the database (category_overrides table)
# No more file-based storage!

@app.route('/api/auth/register', methods=['POST'])
def register():
    """
    Register a new user
    """
    try:
        data = request.get_json()
        email = data.get('email', '').strip()
        password = data.get('password', '')
        name = data.get('name', '').strip()
        
        if not email or not password or not name:
            return {'error': 'Email, password, and name are required'}, 400
        
        # Generate unique tenant_id for new user (use email hash for uniqueness)
        import hashlib
        tenant_id = hashlib.sha256(email.encode()).hexdigest()[:16]
        
        success, user_data, error = user_manager.register_user(email, password, name, tenant_id=tenant_id)
        
        if not success:
            return {'error': error}, 400
        
        return {
            'success': True,
            'message': 'Registration successful. Please check your email to verify your account.',
            'user': user_data
        }
        
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"Registration error: {e}")
        print(f"Traceback: {error_trace}")
        return {'error': f'Registration failed: {str(e)}'}, 500


@app.route('/api/auth/login', methods=['POST'])
def login():
    """
    Authenticate user and create session token with email and password
    """
    try:
        data = request.get_json()
        email = data.get('email', '').strip() or data.get('username', '').strip()  # Support both for backward compatibility
        password = data.get('password', '')

        if not email or not password:
            return {'error': 'Email and password are required'}, 400

        # Try new authentication system first
        success, user_data, error = user_manager.authenticate_user(email, password)
        
        if success:
            # Create session token
            session_token = session_manager.create_session(
                user_id=str(user_data['id']),
                tenant_id=user_data['tenant_id'],
                additional_claims={
                    'email': user_data['email'],
                    'name': user_data['name'],
                    'email_verified': user_data['email_verified']
                }
            )

            return {
                'success': True,
                'session_token': session_token,
                'user': {
                    'id': user_data['id'],
                    'email': user_data['email'],
                    'name': user_data['name'],
                    'tenant': user_data['tenant_id'],
                    'email_verified': user_data['email_verified']
                }
            }
        
        # Fallback to demo user for backward compatibility
        if email == 'demo@demo' and password == 'demo':
            session_token = session_manager.create_session(
                user_id='demo',
                tenant_id='default',
                additional_claims={'role': 'user', 'email': 'demo@example.com'}
            )
            return {
                'success': True,
                'session_token': session_token,
                'user': {'id': 'demo', 'email': 'demo@example.com', 'name': 'Demo User'}
            }
        
        return {'error': error or 'Invalid email or password'}, 401
        
    except Exception as e:
        print(f"Login error: {e}")
        import traceback
        traceback.print_exc()
        return {'error': 'Login failed'}, 500


@app.route('/api/auth/request-password-reset', methods=['POST'])
def request_password_reset():
    """
    Request a password reset email
    """
    try:
        data = request.get_json()
        email = data.get('email', '').strip()
        
        if not email:
            return {'error': 'Email is required'}, 400
        
        success, error = user_manager.request_password_reset(email)
        
        # Always return success to prevent email enumeration
        return {
            'success': True,
            'message': 'If an account exists with this email, a password reset link will be sent.'
        }
    except Exception as e:
        print(f"Password reset request error: {e}")
        return {'error': 'Failed to process password reset request'}, 500


@app.route('/api/auth/reset-password', methods=['POST'])
def reset_password():
    """
    Reset password with token
    """
    try:
        data = request.get_json()
        token = data.get('token', '')
        new_password = data.get('password', '')
        
        if not token or not new_password:
            return {'error': 'Token and new password are required'}, 400
        
        success, error = user_manager.reset_password(token, new_password)
        
        if not success:
            return {'error': error}, 400
        
        return {
            'success': True,
            'message': 'Password reset successful. You can now log in with your new password.'
        }
    except Exception as e:
        print(f"Password reset error: {e}")
        return {'error': 'Failed to reset password'}, 500


@app.route('/api/auth/verify-email', methods=['POST'])
def verify_email():
    """
    Verify email address with token
    """
    try:
        data = request.get_json()
        token = data.get('token', '')
        
        if not token:
            return {'error': 'Verification token is required'}, 400
        
        success, error = user_manager.verify_email(token)
        
        if not success:
            return {'error': error}, 400
        
        return {
            'success': True,
            'message': 'Email verified successfully!'
        }
    except Exception as e:
        print(f"Email verification error: {e}")
        return {'error': 'Failed to verify email'}, 500


@app.route('/api/auth/verify', methods=['GET'])
@authenticate_request
def verify_session():
    """Verify current session is valid"""
    if g.session_claims:
        return {
            'valid': True,
            'user': {
                'id': g.session_claims.get('sub'),
                'tenant': g.session_claims.get('tenant')
            }
        }
    else:
        return {'valid': False}, 401


def _load_categories(filename):
    """Load category definitions from JSON file"""
    filepath = os.path.join(os.path.dirname(__file__), filename)
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Warning: {filename} not found. Using default categories.")
        return {}
    except json.JSONDecodeError as e:
        print(f"Error parsing {filename}: {e}. Using default categories.")
        return {}

class BankStatementParser:
    def __init__(self):
        self.transactions = []
        self.account_balances = {}  # Store actual account balances

    def _detect_dkb_account_type(self, lines):
        """Detect DKB account type from file content"""
        # Check first few lines for account type
        for line in lines[:5]:
            if 'Girokonto' in line:
                return 'DKB Girokonto'
            elif 'Tagesgeld' in line:
                return 'DKB Tagesgeld'
        return 'DKB'

    def _extract_dkb_balance(self, lines):
        """Extract current balance from DKB CSV header"""
        for line in lines[:10]:
            if 'Kontostand vom' in line:
                # Line format: "Kontostand vom 19.10.2025:";"2.685,35 ‚Ç¨"
                parts = line.split(';')
                if len(parts) >= 2:
                    # Remove quotes, euro symbol, non-breaking spaces, and regular spaces
                    balance_str = parts[1].strip().strip('"').replace('‚Ç¨', '').replace('\xa0', '').strip()
                    # Convert German number format (1.000,00) to float
                    balance_str = balance_str.replace('.', '').replace(',', '.')
                    try:
                        return float(balance_str)
                    except ValueError as e:
                        print(f"Error parsing DKB balance: {e}, string was: {repr(balance_str)}")
                        return None
        return None

    def parse_dkb(self, filepath, account_name=None):
        """Parse DKB German bank statements (EUR)"""
        transactions = []
        with open(filepath, 'r', encoding='utf-8-sig') as f:
            lines = f.readlines()

            # Auto-detect account type if not provided
            if account_name is None:
                account_name = self._detect_dkb_account_type(lines)

            # Extract current balance from header
            balance = self._extract_dkb_balance(lines)
            if balance is not None:
                self.account_balances[account_name] = {
                    'balance': balance,
                    'currency': 'EUR'
                }
            
            # Find the CSV header row (starts with "Buchungstag", "Buchungsdatum", or "Buchung")
            header_idx = None
            for i, line in enumerate(lines):
                if 'Buchungstag' in line or 'Buchungsdatum' in line or 'Buchung' in line:
                    header_idx = i
                    break
            
            if header_idx is None:
                print(f"Warning: Could not find CSV header in DKB file {filepath}")
                return transactions
            
            # Parse CSV starting from header
            reader = csv.DictReader(lines[header_idx:], delimiter=';')
            print(f"CSV reader fieldnames: {reader.fieldnames}")
            
            for row_num, row in enumerate(reader, start=1):
                try:
                    # Debug: print first few rows
                    if row_num <= 3:
                        print(f"Row {row_num}: {dict(row)}")
                    
                    # DKB CSV format has multiple variants:
                    # Old: Buchungstag, Wertstellung, Buchungstext, Empf√§nger/Auftraggeber, Verwendungszweck, Betrag, W√§hrung
                    # New: Buchungsdatum, Wertstellung, Status, Zahlungspflichtige*r, Zahlungsempf√§nger*in, Verwendungszweck, Umsatztyp, IBAN, Betrag (‚Ç¨), ...
                    
                    # Try different date column names (handle both quoted and unquoted)
                    date_str = (row.get('Buchungstag', '').strip() or 
                               row.get('Buchungsdatum', '').strip() or 
                               row.get('Buchung', '').strip() or
                               row.get('"Buchungstag"', '').strip() or
                               row.get('"Buchungsdatum"', '').strip()).strip('"')
                    if not date_str:
                        if row_num <= 3:
                            print(f"  No date found in row {row_num}")
                        continue
                    
                    # Parse date - handle both DD.MM.YYYY and DD.MM.YY formats
                    date = None
                    try:
                        date = datetime.strptime(date_str, '%d.%m.%Y')
                    except ValueError:
                        try:
                            date = datetime.strptime(date_str, '%d.%m.%y')  # 2-digit year
                        except ValueError:
                            try:
                                date = datetime.strptime(date_str, '%Y-%m-%d')
                            except ValueError:
                                if row_num <= 3:
                                    print(f"  Warning: Could not parse date '{date_str}'")
                                continue
                    
                    # Try different amount column names (handle both quoted and unquoted)
                    amount_str = (row.get('Betrag', '').strip() or 
                                 row.get('Betrag (‚Ç¨)', '').strip() or
                                 row.get('"Betrag"', '').strip() or
                                 row.get('"Betrag (‚Ç¨)"', '').strip()).strip('"')
                    if not amount_str:
                        if row_num <= 3:
                            print(f"  No amount found in row {row_num}")
                        continue
                    
                    # Parse amount (German format: 1.234,56 or -1.234,56)
                    amount_str = amount_str.replace('.', '').replace(',', '.')
                    try:
                        amount = float(amount_str)
                    except ValueError:
                        if row_num <= 3:
                            print(f"  Warning: Could not parse amount '{amount_str}'")
                        continue
                    
                    # Get currency (default to EUR for DKB)
                    currency = row.get('W√§hrung', 'EUR').strip().strip('"')
                    if not currency:
                        currency = 'EUR'
                    
                    # Try different recipient/sender column names
                    recipient = (row.get('Empf√§nger/Auftraggeber', '').strip() or 
                                row.get('Empf√§nger', '').strip() or
                                row.get('Zahlungsempf√§nger*in', '').strip() or
                                row.get('Zahlungspflichtige*r', '').strip() or
                                row.get('"Zahlungsempf√§nger*in"', '').strip() or
                                row.get('"Zahlungspflichtige*r"', '').strip()).strip('"')
                    
                    # Get description
                    description = (row.get('Verwendungszweck', '').strip() or 
                                  row.get('Buchungstext', '').strip() or
                                  row.get('"Verwendungszweck"', '').strip()).strip('"')
                    
                    # Determine transaction type (positive = income, negative = expense)
                    transaction_type = 'income' if amount > 0 else 'expense'
                    
                    # Categorize transaction
                    category = self._categorize_transaction(recipient, description, date.strftime('%Y-%m-%d'), account_name)
                    
                    transactions.append({
                        'date': date.isoformat(),
                        'amount': abs(amount),
                        'currency': currency,
                        'recipient': recipient,
                        'description': description,
                        'category': category,
                        'type': transaction_type,
                        'account': account_name
                    })
                except (ValueError, KeyError) as e:
                    print(f"Error parsing DKB row {row_num}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
        
        print(f"Parsed {len(transactions)} transactions from DKB file {filepath}")
        return transactions

    def parse_yuh(self, filepath):
        """Parse YUH Swiss bank statements (CHF)"""
        transactions = []
        yuh_main_balance = 0
        yuh_goal_balances = defaultdict(float)

        with open(filepath, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f, delimiter=';')
            for row in reader:
                try:
                    # Parse date format (DD/MM/YYYY)
                    date_str = row.get('DATE', '').strip()
                    if not date_str:
                        continue

                    date = datetime.strptime(date_str, '%d/%m/%Y')

                    # Determine amount (DEBIT or CREDIT) - only count CHF transactions
                    debit = row.get('DEBIT', '').strip()
                    credit = row.get('CREDIT', '').strip()
                    debit_currency = row.get('DEBIT CURRENCY', '').strip()
                    credit_currency = row.get('CREDIT CURRENCY', '').strip()

                    # Only process CHF transactions for balance calculation
                    if debit and debit_currency == 'CHF':
                        amount = float(debit)
                    elif credit and credit_currency == 'CHF':
                        amount = float(credit)
                    else:
                        # Skip non-CHF transactions (USD, EUR investments, etc.)
                        continue

                    # Get activity details
                    activity_name = row.get('ACTIVITY NAME', '').strip('"')
                    recipient = row.get('RECIPIENT', '').strip('"')
                    locality = row.get('LOCALITY', '').strip('"')

                    # Handle goal deposits and withdrawals for virtual savings accounts
                    activity_type = row.get('ACTIVITY TYPE', '')

                    # Track goal account balances (internal transfers within YUH)
                    if activity_type == 'GOAL_DEPOSIT':
                        # Money moved from main account to goal account
                        # CREDIT column has positive amount
                        goal_name = recipient or locality
                        if goal_name:
                            # Strip quotes from goal name
                            goal_name = goal_name.strip('"')
                            yuh_goal_balances[goal_name] += amount  # add to goal
                            # Don't change main balance - goal deposits are internal transfers
                        continue
                    elif activity_type == 'GOAL_WITHDRAWAL':
                        # Money moved from goal account back to main account
                        # DEBIT column has negative amount
                        goal_name = recipient or locality
                        if goal_name:
                            # Strip quotes from goal name
                            goal_name = goal_name.strip('"')
                            yuh_goal_balances[goal_name] += amount  # subtract from goal (amount is negative)
                            # Don't change main balance - goal withdrawals are internal transfers
                        continue
                    elif activity_type == 'REWARD_RECEIVED':
                        # Skip rewards (these are in different currency - SWQ tokens)
                        continue

                    # All other transactions affect main YUH balance
                    yuh_main_balance += amount

                    category = self._categorize_transaction(recipient or activity_name, locality, date.strftime('%Y-%m-%d'), 'YUH')

                    transactions.append({
                        'date': date.isoformat(),
                        'amount': amount,
                        'currency': 'CHF',
                        'recipient': recipient or activity_name,
                        'description': f"{activity_name} {locality}".strip(),
                        'category': category,
                        'type': 'income' if amount > 0 else 'expense',
                        'account': 'YUH'
                    })
                except (ValueError, KeyError) as e:
                    continue

        # Calculate total across all goal accounts
        total_goal_balance = sum(yuh_goal_balances.values())

        # YUH main balance = total balance - money locked in goals
        yuh_main_account_balance = yuh_main_balance - total_goal_balance

        # Store YUH main account balance
        self.account_balances['YUH'] = {
            'balance': yuh_main_account_balance,
            'currency': 'CHF'
        }

        # Store YUH goal account balances as virtual accounts
        for goal_name, goal_balance in yuh_goal_balances.items():
            account_name = f'YUH - {goal_name}'
            self.account_balances[account_name] = {
                'balance': goal_balance,
                'currency': 'CHF'
            }

        return transactions

    def parse_viac(self, filepath):
        """Parse VIAC broker statements from PDF files"""
        transactions = []

        try:
            reader = PdfReader(filepath)
            text = ""
            for page in reader.pages:
                text += page.extract_text()

            # Extract key information using regex patterns
            # Date pattern: Basel, DD.MM.YYYY
            date_match = re.search(r'Basel,\s*(\d{2}\.\d{2}\.\d{4})', text)
            if not date_match:
                return transactions

            date_str = date_match.group(1)
            date = datetime.strptime(date_str, '%d.%m.%Y')

            # Detect transaction type: "B√∂rsenabrechnung - Kauf" or "B√∂rsenabrechnung - Verkauf"
            transaction_type_match = re.search(r'B√∂rsenabrechnung\s*-\s*(Kauf|Verkauf)', text)
            is_sell = transaction_type_match and transaction_type_match.group(1) == 'Verkauf'
            transaction_type = 'sell' if is_sell else 'buy'

            # Extract security name from title "B√∂rsenabrechnung - Kauf/Verkauf [Name]"
            security_match = re.search(r'B√∂rsenabrechnung\s*-\s*(?:Kauf|Verkauf)\s+(.+?)(?:\n|Wir haben)', text, re.DOTALL)
            security_name = security_match.group(1).strip() if security_match else 'Unknown'

            # Extract shares: "X.XXX Anteile [Name]"
            # In Swiss format, the decimal point is used for decimals (e.g., 8.008)
            shares_match = re.search(r'(\d+\.\d+)\s+Anteile', text)
            shares = float(shares_match.group(1)) if shares_match else 0

            # Extract ISIN
            isin_match = re.search(r'ISIN:\s*([A-Z0-9]+)', text)
            isin = isin_match.group(1) if isin_match else ''

            # Extract price in USD
            # Swiss format uses apostrophes for thousands and periods for decimals
            price_match = re.search(r'Kurs:\s*USD\s*([\d\'\.]+)', text)
            price_usd = 0
            if price_match:
                price_str = price_match.group(1).replace('\'', '')  # Remove apostrophes (thousands separator)
                price_usd = float(price_str)

            # Extract total amount in CHF (Verrechneter Betrag)
            # Swiss format uses apostrophes for thousands (3'052.94) and periods for decimals
            amount_match = re.search(r'Verrechneter Betrag:.*?CHF\s*([\d\'\.]+)', text, re.DOTALL)
            amount_chf = 0
            if amount_match:
                amount_str = amount_match.group(1).replace('\'', '')  # Remove apostrophes (thousands separator)
                amount_chf = float(amount_str)

            # Extract valuta date
            valuta_match = re.search(r'Valuta\s*(\d{2}\.\d{2}\.\d{4})', text)
            valuta_date = date
            if valuta_match:
                valuta_date = datetime.strptime(valuta_match.group(1), '%d.%m.%Y')

            # Create transaction record
            # For buys: negative amount (outflow), positive shares
            # For sells: positive amount (inflow), positive shares (aggregation handles sign)
            transactions.append({
                'date': valuta_date.isoformat(),
                'security': security_name,
                'isin': isin,
                'shares': shares,  # Always positive, aggregation logic handles buy/sell
                'price_usd': price_usd,
                'amount': amount_chf if is_sell else -amount_chf,  # Positive for sells (inflow), negative for buys (outflow)
                'currency': 'CHF',
                'type': transaction_type,
                'account': 'VIAC'
            })

        except Exception as e:
            print(f"Error parsing VIAC PDF {filepath}: {e}")

        return transactions

    def parse_ing_diba(self, filepath):
        """Parse ING DiBa broker depot overview from CSV files"""
        holdings = []

        try:
            # Try different encodings for German characters
            encodings = ['utf-8-sig', 'windows-1252', 'iso-8859-1', 'cp1252']
            lines = None

            for encoding in encodings:
                try:
                    with open(filepath, 'r', encoding=encoding) as f:
                        lines = f.readlines()
                    break
                except UnicodeDecodeError:
                    continue

            if lines is None:
                print(f"Could not decode file {filepath} with any known encoding")
                return holdings

            # Extract date from first line: "Depot√ºbersicht vom DD.MM.YYYY HH:MM"
            date_match = re.search(r'vom\s+(\d{2}\.\d{2}\.\d{4})', lines[0])
            snapshot_date = datetime.now()
            if date_match:
                snapshot_date = datetime.strptime(date_match.group(1), '%d.%m.%Y')

            # Find the header line (contains "ISIN")
            header_idx = None
            for i, line in enumerate(lines):
                if 'ISIN' in line and 'Wertpapiername' in line:
                    header_idx = i
                    break

            if header_idx is None:
                return holdings

            # Parse CSV starting from header
            reader = csv.DictReader(lines[header_idx:], delimiter=';')
            for row in reader:
                try:
                    # Skip summary row (empty ISIN)
                    isin = row.get('ISIN', '').strip().strip('"')
                    if not isin:
                        continue

                    # Extract security name
                    security = row.get('Wertpapiername', '').strip().strip('"')

                    # Extract shares - German format uses comma for decimal (167 or 1)
                    shares_str = row.get('St√ºck/Nominale', '').strip().strip('"')
                    if not shares_str:
                        continue
                    shares = float(shares_str.replace('.', '').replace(',', '.'))

                    # Extract purchase value (Einstandswert) - German format: 34.939,89
                    purchase_value_str = row.get('Einstandswert', '').strip().strip('"')
                    if not purchase_value_str:
                        continue
                    purchase_value = float(purchase_value_str.replace('.', '').replace(',', '.'))

                    # Extract currency
                    currency = row.get('W√§hrung', 'EUR').strip().strip('"')
                    # There are multiple currency columns, get the one after Einstandswert
                    # We'll use the 7th column which is the currency for Einstandswert

                    # Extract current value (Kurswert)
                    current_value_str = row.get('Kurswert', '').strip().strip('"')
                    current_value = 0
                    if current_value_str:
                        current_value = float(current_value_str.replace('.', '').replace(',', '.'))

                    # Extract average purchase price (Einstandskurs)
                    avg_price_str = row.get('Einstandskurs', '').strip().strip('"')
                    avg_price = 0
                    if avg_price_str:
                        avg_price = float(avg_price_str.replace('.', '').replace(',', '.'))

                    holdings.append({
                        'isin': isin,
                        'security': security,
                        'shares': shares,
                        'total_cost': purchase_value,
                        'current_value': current_value,
                        'average_cost': avg_price,
                        'account': 'ING DiBa',
                        'date': snapshot_date.isoformat(),
                        'purchase_date': '2024-01-16'  # ING DiBa purchase date
                    })

                except (ValueError, KeyError) as e:
                    print(f"Error parsing ING DiBa row: {e}, row: {row}")
                    continue

        except Exception as e:
            print(f"Error parsing ING DiBa CSV {filepath}: {e}")

        return holdings

    def parse_kfw(self, filepath):
        """Parse KfW student loan statements from PDF files"""
        loans = []

        try:
            reader = PdfReader(filepath)
            text = ""
            for page in reader.pages:
                text += page.extract_text()

            # Extract key information using regex patterns
            # Date pattern: "Kontoauszug per DD.MM.YYYY"
            date_match = re.search(r'Kontoauszug per (\d{2}\.\d{2}\.\d{4})', text)
            if not date_match:
                return loans

            statement_date = datetime.strptime(date_match.group(1), '%d.%m.%Y')

            # Extract loan program type
            program_match = re.search(r'Kreditprogramm:\s*(.+)', text)
            program = program_match.group(1).strip() if program_match else 'Unknown'

            # Extract account number
            account_match = re.search(r'Darlehenskonto-Nr\.:\s*(\d+)', text)
            account_number = account_match.group(1) if account_match else ''

            # Extract contract date
            contract_match = re.search(r'Darlehensvertrag vom:\s*(\d{2}\.\d{2}\.\d{4})', text)
            contract_date = None
            if contract_match:
                contract_date = datetime.strptime(contract_match.group(1), '%d.%m.%Y')

            # Extract current balance - look for the most recent "Kapitalsaldo zum" or "Kontostand per"
            balance_matches = re.findall(r'(?:Kapitalsaldo zum|Kontostand per)\s+\d{2}\.\d{2}\.\d{4}\s+([\d.,]+)', text)
            current_balance = 0
            if balance_matches:
                # Get the last (most recent) balance
                balance_str = balance_matches[-1].replace('.', '').replace(',', '.')
                current_balance = float(balance_str)

            # Extract interest rate
            interest_match = re.search(r'ab \d{2}\.\d{2}\.\d{4}:\s*([\d.,]+)\s*%', text)
            interest_rate = 0
            if interest_match:
                interest_str = interest_match.group(1).replace(',', '.')
                interest_rate = float(interest_str)

            # Extract monthly payment if available
            payment_match = re.search(r'Lastschrift\s+([\d.,]+)', text)
            monthly_payment = 0
            if payment_match:
                payment_str = payment_match.group(1).replace('.', '').replace(',', '.')
                monthly_payment = float(payment_str)

            # Extract deferred interest if available
            deferred_interest_match = re.search(r'Aufgeschobene Zinsen:\s*([\d.,]+)\s*EUR', text)
            deferred_interest = 0
            if deferred_interest_match:
                deferred_str = deferred_interest_match.group(1).replace('.', '').replace(',', '.')
                deferred_interest = float(deferred_str)

            # Create loan record
            loan_data = {
                'account_number': account_number,
                'program': program,
                'current_balance': current_balance,
                'interest_rate': interest_rate,
                'monthly_payment': monthly_payment,
                'deferred_interest': deferred_interest,
                'statement_date': statement_date.isoformat(),
                'contract_date': contract_date.isoformat() if contract_date else None,
                'account': f'KfW {program}',
                'type': 'loan'
            }
            loans.append(loan_data)

        except Exception as e:
            print(f"Error parsing KfW PDF {filepath}: {e}")

        return loans

    def _categorize_transaction(self, recipient, description, date=None, account=None):
        """Categorize transaction based on recipient and description
        
        Note: This is only used during initial parsing of bank statements.
        Category overrides are now stored directly in the database.
        """
        recipient = (recipient or '').strip()
        description = (description or '').strip()
        if date:
            date = date.split('T')[0] if 'T' in date else date
        else:
            date = ''
        account = account or ''
        text = f"{recipient} {description}".strip().lower()

        # Load categories fresh each time to pick up changes
        spending_categories = _load_categories('categories_spending.json')
        income_categories = _load_categories('categories_income.json')
        internal_transfer_config = _load_categories('categories_internal_transfer.json')

        # Check for initial setup transactions first (these should be completely excluded)
        if internal_transfer_config and date and account:
            initial_setup_transactions = internal_transfer_config.get('initial_setup', [])
            for setup_transaction in initial_setup_transactions:
                if (setup_transaction.get('date') == date and
                    setup_transaction.get('account') == account and
                    setup_transaction.get('description', '').lower() in description.lower()):
                    return 'Internal Transfer'

        # Check for internal transfers first (these should be excluded from income/expenses)
        if internal_transfer_config:
            # Check for money transfer services (Wise, Exchange Market)
            transfer_keywords = internal_transfer_config.get('keywords', [])
            if any(keyword.lower() in text for keyword in transfer_keywords):
                return 'Internal Transfer'

            # Check for self-transfers (same person sender and recipient)
            self_patterns = internal_transfer_config.get('self_transfer_patterns', [])
            if self_patterns:
                for pattern in self_patterns:
                    pattern_lower = pattern.lower()
                    recipient_lower = recipient.lower()
                    description_lower = description.lower() if description else ''

                    recipient_match = pattern_lower in recipient_lower

                    # Check if all words from pattern appear in description (more flexible matching)
                    pattern_words = set(pattern_lower.split())
                    description_words = set(description_lower.split())
                    description_match = pattern_words.issubset(description_words) if description else False

                    # If both recipient and description contain the same name pattern, it's a self-transfer
                    # OR if recipient matches and description is empty/very short
                    if (recipient_match and description_match) or (recipient_match and (not description or len(description.strip()) < 10)):
                        return 'Internal Transfer'

        # Check spending categories from config file
        for category, keywords in spending_categories.items():
            if any(keyword.lower() in text for keyword in keywords):
                return category

        # Check income categories from config file
        for category, keywords in income_categories.items():
            if any(keyword.lower() in text for keyword in keywords):
                return category

        # Check for transfers (special handling to exclude certain keywords)
        transfer_keywords = ['√ºberweisung', 'twint', 'paypal']
        exclude_keywords = ['apple'] + [kw.lower() for keywords in income_categories.values() for kw in keywords]

        if any(word in text for word in transfer_keywords) and not any(word in text for word in exclude_keywords):
            return 'Transfer'

        return 'Other'


@app.route('/api/transactions')
@authenticate_request
@require_auth
def get_transactions():
    """Get transactions from database"""
    tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'

    try:
        print(f"Getting transactions for tenant: {tenant_id}")
        transactions = wealth_db.get_transactions(tenant_id)
        print(f"Found {len(transactions)} transactions")
        return jsonify(transactions)
    except Exception as e:
        import traceback
        print(f"Error getting transactions: {e}")
        print(traceback.format_exc())
        return jsonify({'error': 'Failed to retrieve transactions'}), 500

@app.route('/api/summary')
@authenticate_request
@require_auth
def get_summary():
    """Get transaction summary from database"""
    tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
    print(f"üìä get_summary called with tenant_id: {tenant_id}")

    try:
        # Get all transactions from database
        db_transactions = wealth_db.get_transactions(tenant_id, limit=10000, offset=0)

        # No demo data - users must upload their own data
        print(f"Total transactions from database: {len(db_transactions)}")
        
        if len(db_transactions) > 0:
            # Convert database transactions to the format expected by the frontend
            transactions = []
            for t in db_transactions:
                transactions.append({
                    'date': t['transaction_date'].isoformat() if hasattr(t['transaction_date'], 'isoformat') else str(t['transaction_date']),
                    'amount': float(t['amount']) if t['transaction_type'] == 'income' else -float(t['amount']),
                    'currency': t['currency'],
                    'type': t['transaction_type'],
                    'recipient': t.get('recipient', ''),
                    'description': t.get('description', ''),
                    'category': t.get('category', 'Uncategorized'),
                    'account': t.get('account_name', 'Unknown'),
                    'transaction_hash': t.get('transaction_hash', '')
                })

            print(f"Formatted {len(transactions)} transactions for frontend")
        else:
            # No transactions found - return empty array (user will see onboarding)
            print(f"No transactions found for tenant {tenant_id} - returning empty array")
            return jsonify([])
    except Exception as e:
        print(f"Error fetching transactions from database: {e}")
        import traceback
        traceback.print_exc()
        # On error, return empty array (user will see onboarding)
        return jsonify([])

    # Group by month
    monthly_data = defaultdict(lambda: {
        'income': 0,
        'expenses': 0,
        'income_categories': defaultdict(float),
        'income_transactions': defaultdict(list),
        'expense_categories': defaultdict(float),
        'expense_transactions': defaultdict(list),
        'internal_transfer_total': 0,
        'internal_transfer_transactions': [],
        'currency_totals': {'EUR': 0, 'CHF': 0}
    })

    print(f"Processing {len(transactions)} transactions for grouping...")
    for idx, t in enumerate(transactions):
        try:
            # Parse date - handle different formats
            date_str = t['date']
            if isinstance(date_str, str):
                # Try ISO format first
                try:
                    date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                except ValueError:
                    # Try other formats
                    try:
                        date = datetime.strptime(date_str, '%Y-%m-%d')
                    except ValueError:
                        try:
                            date = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
                        except ValueError:
                            print(f"Warning: Could not parse date '{date_str}' for transaction {idx}")
                            continue
            else:
                # Already a datetime object
                date = date_str
            
            month_key = date.strftime('%Y-%m')

            # Track internal transfers separately (don't include in income/expense calculations)
            if t['category'] == 'Internal Transfer':
                monthly_data[month_key]['internal_transfer_total'] += abs(t['amount'])
                monthly_data[month_key]['internal_transfer_transactions'].append({
                    'date': t['date'],
                    'amount': t['amount'],
                    'currency': t['currency'],
                    'recipient': t['recipient'],
                    'description': t['description'],
                    'account': t['account'],
                    'transaction_hash': t.get('transaction_hash', '')
                })
                monthly_data[month_key]['currency_totals'][t['currency']] += t['amount']
                continue

            if t['type'] == 'income':
                monthly_data[month_key]['income'] += abs(t['amount'])
                monthly_data[month_key]['income_categories'][t['category']] += abs(t['amount'])
                # Store individual income transactions for each category
                monthly_data[month_key]['income_transactions'][t['category']].append({
                    'date': t['date'],
                    'amount': t['amount'],
                    'currency': t['currency'],
                    'recipient': t['recipient'],
                    'description': t['description'],
                    'account': t['account'],
                    'transaction_hash': t.get('transaction_hash', '')
                })
            else:
                monthly_data[month_key]['expenses'] += abs(t['amount'])
                monthly_data[month_key]['expense_categories'][t['category']] += abs(t['amount'])
                # Store individual expense transactions for each category
                monthly_data[month_key]['expense_transactions'][t['category']].append({
                    'date': t['date'],
                    'amount': t['amount'],
                    'currency': t['currency'],
                    'recipient': t['recipient'],
                    'description': t['description'],
                    'account': t['account'],
                    'transaction_hash': t.get('transaction_hash', '')
                })

            monthly_data[month_key]['currency_totals'][t['currency']] += t['amount']
        except Exception as e:
            print(f"Error processing transaction {idx}: {e}")
            print(f"Transaction data: {t}")
            import traceback
            traceback.print_exc()
            continue

    print(f"Grouped transactions into {len(monthly_data)} months")
    
    # Generate predictions for current month
    current_month = datetime.now().strftime('%Y-%m')
    try:
        print(f"Generating predictions for current month: {current_month}")
        
        # Detect recurring patterns
        detector = RecurringPatternDetector()
        patterns = detector.detect_recurring_patterns(transactions)
        print(f"Detected {len(patterns)} recurring patterns")
        
        # Get dismissed predictions
        dismissed_predictions = set()
        try:
            # Query dismissed predictions directly
            target_year, target_month_num = map(int, current_month.split('-'))
            target_date = datetime(target_year, target_month_num, 1)
            
            with wealth_db.db.get_cursor() as cursor:
                # Check if table exists
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'prediction_dismissals'
                    )
                """)
                table_exists = cursor.fetchone()[0]
                
                if table_exists:
                    cursor.execute("""
                        SELECT prediction_key 
                        FROM prediction_dismissals 
                        WHERE tenant_id = (SELECT id FROM tenants WHERE tenant_id = %s)
                        AND (expires_at IS NULL OR expires_at >= %s)
                    """, [tenant_id, target_date.date()])
                    result = cursor.fetchall()
                    dismissed_predictions = {row[0] for row in result}
                    print(f"Found {len(dismissed_predictions)} dismissed predictions")
                else:
                    print("‚ö†Ô∏è prediction_dismissals table does not exist yet (run schema.sql to create it)")
        except Exception as e:
            print(f"Warning: Could not fetch dismissed predictions: {e}")
            import traceback
            traceback.print_exc()
        
        # Generate predictions for current month
        predictions = detector.generate_predictions_for_month(patterns, current_month, dismissed_predictions)
        print(f"Generated {len(predictions)} predictions for {current_month}")
        
        # Debug: Print prediction details
        for pred in predictions:
            print(f"  - Prediction: {pred['recipient']} ({pred['category']}) - {pred['recurrence_type']} - {pred['amount']} {pred['currency']}")
        
        # Merge predictions into current month's data
        if predictions:
            # Create current month entry if it doesn't exist
            if current_month not in monthly_data:
                monthly_data[current_month] = {
                    'income': 0,
                    'expenses': 0,
                    'income_categories': defaultdict(float),
                    'income_transactions': defaultdict(list),
                    'expense_categories': defaultdict(float),
                    'expense_transactions': defaultdict(list),
                    'internal_transfer_total': 0,
                    'internal_transfer_transactions': [],
                    'currency_totals': {'EUR': 0, 'CHF': 0}
                }
            
            print(f"üí° Adding {len(predictions)} predictions to month {current_month}")
            print(f"üí° Current month data has {len(monthly_data[current_month]['income_transactions'])} income categories and {len(monthly_data[current_month]['expense_transactions'])} expense categories")
            
            # Filter out predictions that match existing transactions
            filtered_predictions = []
            for prediction in predictions:
                category = prediction['category']
                recipient = prediction['recipient']
                pred_amount = abs(float(prediction['amount']))
                
                # Check if there's already a matching transaction this month
                existing_transactions = []
                if prediction['type'] == 'income':
                    existing_transactions = monthly_data[current_month]['income_transactions'].get(category, [])
                else:
                    existing_transactions = monthly_data[current_month]['expense_transactions'].get(category, [])
                
                # Look for a matching transaction (same recipient and similar amount within 10%)
                is_duplicate = False
                for txn in existing_transactions:
                    if txn.get('recipient') == recipient:
                        txn_amount = abs(float(txn.get('amount', 0)))
                        amount_diff = abs(txn_amount - pred_amount) / pred_amount if pred_amount > 0 else 0
                        if amount_diff < 0.10:  # Within 10% of predicted amount
                            is_duplicate = True
                            print(f"  ‚è≠Ô∏è  Skipping prediction for {recipient}: actual transaction already exists ({txn_amount} vs predicted {pred_amount})")
                            break
                
                if not is_duplicate:
                    filtered_predictions.append(prediction)
            
            print(f"üí° After filtering, {len(filtered_predictions)} predictions remain (removed {len(predictions) - len(filtered_predictions)} duplicates)")
            
            for prediction in filtered_predictions:
                category = prediction['category']
                amount = abs(float(prediction['amount']))
                
                if prediction['type'] == 'income':
                    # Add to income category transactions
                    monthly_data[current_month]['income_transactions'][category].append(prediction)
                    # Add predicted amount to totals
                    monthly_data[current_month]['income'] += amount
                    monthly_data[current_month]['income_categories'][category] += amount
                    print(f"  ‚úì Added income prediction to category '{category}': {amount} (now has {len(monthly_data[current_month]['income_transactions'][category])} transactions)")
                else:
                    # Add to expense category transactions
                    monthly_data[current_month]['expense_transactions'][category].append(prediction)
                    # Add predicted amount to totals
                    monthly_data[current_month]['expenses'] += amount
                    monthly_data[current_month]['expense_categories'][category] += amount
                    print(f"  ‚úì Added expense prediction to category '{category}': {amount} (now has {len(monthly_data[current_month]['expense_transactions'][category])} transactions)")
    except Exception as e:
        print(f"Error generating predictions: {e}")
        import traceback
        traceback.print_exc()
    
    # Calculate saving rate for each month
    summary = []
    for month, data in sorted(monthly_data.items(), reverse=True):
        total_income = data['income']
        total_expenses = data['expenses']
        savings = total_income - total_expenses
        saving_rate = (savings / total_income * 100) if total_income > 0 else 0

        # Prepare income category data with transactions
        # Include categories that have transactions even if total is 0 (for predictions)
        income_categories_with_transactions = {}
        all_income_categories = set(data['income_categories'].keys()) | set(data['income_transactions'].keys())
        for category in all_income_categories:
            total = data['income_categories'].get(category, 0)
            transactions = data['income_transactions'].get(category, [])
            if transactions:  # Only include if there are transactions
                income_categories_with_transactions[category] = {
                    'total': round(total, 2),
                    'transactions': sorted(
                        transactions,
                        key=lambda x: x['date'],
                        reverse=True
                    )
                }

        # Prepare expense category data with transactions
        # Include categories that have transactions even if total is 0 (for predictions)
        expense_categories_with_transactions = {}
        all_expense_categories = set(data['expense_categories'].keys()) | set(data['expense_transactions'].keys())
        for category in all_expense_categories:
            total = data['expense_categories'].get(category, 0)
            transactions = data['expense_transactions'].get(category, [])
            if transactions:  # Only include if there are transactions
                expense_categories_with_transactions[category] = {
                    'total': round(total, 2),
                    'transactions': sorted(
                        transactions,
                        key=lambda x: x['date'],
                        reverse=True
                    )
                }

        # Prepare internal transfers data
        internal_transfers_data = None
        if data['internal_transfer_total'] > 0:
            internal_transfers_data = {
                'total': round(data['internal_transfer_total'], 2),
                'transactions': sorted(
                    data['internal_transfer_transactions'],
                    key=lambda x: x['date'],
                    reverse=True
                )
            }

        month_summary = {
            'month': month,
            'income': round(total_income, 2),
            'expenses': round(total_expenses, 2),
            'savings': round(savings, 2),
            'saving_rate': round(saving_rate, 2),
            'income_categories': income_categories_with_transactions,
            'expense_categories': expense_categories_with_transactions,
            'internal_transfers': internal_transfers_data,
            'currency_totals': {k: round(v, 2) for k, v in data['currency_totals'].items()}
        }
        
        # Log info about current month
        if month == current_month:
            print(f"üìä Current month ({month}) summary:")
            print(f"   Income categories: {len(income_categories_with_transactions)}")
            print(f"   Expense categories: {len(expense_categories_with_transactions)}")
            for cat_name, cat_data in income_categories_with_transactions.items():
                predicted_count = sum(1 for t in cat_data['transactions'] if t.get('is_predicted'))
                if predicted_count > 0:
                    print(f"   üì• Income category '{cat_name}': {len(cat_data['transactions'])} transactions ({predicted_count} predicted)")
            for cat_name, cat_data in expense_categories_with_transactions.items():
                predicted_count = sum(1 for t in cat_data['transactions'] if t.get('is_predicted'))
                if predicted_count > 0:
                    print(f"   üì§ Expense category '{cat_name}': {len(cat_data['transactions'])} transactions ({predicted_count} predicted)")
        
        summary.append(month_summary)

    print(f"Returning summary with {len(summary)} months")
    return jsonify(summary)


@app.route('/api/accounts')
@authenticate_request
@require_auth
def get_accounts():
    """Get accounts from database"""
    tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'

    try:
        accounts = wealth_db.get_accounts(tenant_id)

        accounts_list = []
        totals = {'EUR': 0, 'CHF': 0}

        # Add bank accounts
        if accounts:
            for account in accounts:
                account_summary = {
                    'account': account['account_name'],
                    'balance': float(account['balance']) if account['balance'] else 0,
                    'currency': account['currency'],
                    'transaction_count': 0,
                    'last_transaction_date': None
                }
                accounts_list.append(account_summary)

                currency = account['currency']
                balance = float(account['balance']) if account['balance'] else 0
                if currency in totals:
                    totals[currency] += balance

        # Add broker accounts from broker holdings
        # Calculate broker totals directly (simplified version of get_broker logic)
        try:
            parser = BankStatementParser()
            broker_docs = wealth_db.list_file_attachments(tenant_id, file_types=['broker_viac_pdf', 'broker_ing_diba_csv'])
            
            # Aggregate holdings across all documents first
            all_holdings_dict = {}  # Key: ISIN, Value: {shares, total_cost}
            
            if broker_docs:
                encryption_service = get_encryption_service()
                import tempfile
                
                for doc in broker_docs:
                    tmp_path = None
                    try:
                        full_doc = wealth_db.get_file_attachment(tenant_id, doc['id'])
                        if not full_doc:
                            continue
                        
                        encryption_metadata = full_doc.get('encryption_metadata') or {}
                        if isinstance(encryption_metadata, str):
                            try:
                                encryption_metadata = json.loads(encryption_metadata)
                            except json.JSONDecodeError:
                                continue
                        
                        server_encryption = encryption_metadata.get('server_encryption', {})
                        server_algorithm = server_encryption.get('algorithm', 'AES-256-GCM')
                        
                        if server_algorithm == 'none':
                            decrypted = full_doc['encrypted_data']
                        else:
                            nonce_b64 = server_encryption.get('nonce', '')
                            if not nonce_b64:
                                continue
                            
                            try:
                                nonce = base64.b64decode(nonce_b64)
                            except Exception:
                                continue
                            
                            encrypted_data = EncryptedData(
                                ciphertext=full_doc['encrypted_data'],
                                nonce=nonce,
                                key_version=server_encryption.get('key_version', 1)
                            )
                            
                            verification_metadata = encryption_metadata.copy()
                            if 'server_encryption' in verification_metadata:
                                verification_metadata['server_encryption'] = verification_metadata['server_encryption'].copy()
                                verification_metadata['server_encryption'].pop('nonce', None)
                                verification_metadata['server_encryption'].pop('key_version', None)
                            
                            if 'file_info' in verification_metadata:
                                verification_metadata['file_info'] = verification_metadata['file_info'].copy()
                                verification_metadata['file_info'].pop('checksum', None)
                            
                            metadata_json = json.dumps(verification_metadata, sort_keys=True).encode()
                            try:
                                decrypted = encryption_service.decrypt_data(encrypted_data, tenant_id, metadata_json)
                            except ValueError:
                                continue
                        
                        client_metadata = encryption_metadata.get('client_encryption')
                        is_client_encrypted = (
                            client_metadata 
                            and client_metadata.get('algorithm') != 'none'
                            and client_metadata.get('nonce')
                        )
                        
                        if is_client_encrypted and g.session_token:
                            try:
                                decrypted = encryption_service.decrypt_client_layer(
                                    decrypted, 
                                    g.session_token, 
                                    client_metadata
                                )
                            except Exception:
                                continue
                        
                        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(doc['original_name'])[1]) as tmp:
                            tmp.write(decrypted)
                            tmp_path = tmp.name
                        
                        file_type = doc.get('file_type') or full_doc.get('file_type')
                        
                        if file_type == 'broker_viac_pdf':
                            file_transactions = parser.parse_viac(tmp_path)
                            # Aggregate VIAC holdings across all documents
                            for t in file_transactions:
                                key = f"VIAC_{t['isin']}"
                                if key not in all_holdings_dict:
                                    all_holdings_dict[key] = {
                                        'shares': 0,
                                        'total_cost': 0
                                    }
                                all_holdings_dict[key]['shares'] += t['shares'] if t['type'] == 'buy' else -t['shares']
                                all_holdings_dict[key]['total_cost'] += abs(t['amount']) if t['type'] == 'buy' else -abs(t['amount'])
                                    
                        elif file_type == 'broker_ing_diba_csv':
                            file_holdings = parser.parse_ing_diba(tmp_path)
                            # Aggregate ING DiBa holdings across all documents
                            for holding in file_holdings:
                                isin = holding.get('isin', '')
                                if isin:
                                    key = f"ING_DIBA_{isin}"
                                    if key not in all_holdings_dict:
                                        all_holdings_dict[key] = {
                                            'shares': holding.get('shares', 0),
                                            'total_cost': holding.get('total_cost', 0),
                                            'current_value': holding.get('current_value', holding.get('total_cost', 0))
                                        }
                                    else:
                                        # Aggregate shares and costs
                                        all_holdings_dict[key]['shares'] += holding.get('shares', 0)
                                        all_holdings_dict[key]['total_cost'] += holding.get('total_cost', 0)
                                        all_holdings_dict[key]['current_value'] += holding.get('current_value', holding.get('total_cost', 0))
                        
                        if tmp_path:
                            os.unlink(tmp_path)
                    except Exception as e:
                        print(f"Error processing broker document for accounts: {e}")
                        if tmp_path and os.path.exists(tmp_path):
                            try:
                                os.unlink(tmp_path)
                            except:
                                pass
                        continue
                
                # Calculate totals from aggregated holdings
                viac_total_invested = 0
                ing_diba_total_current = 0
                
                for key, holding in all_holdings_dict.items():
                    if key.startswith('VIAC_'):
                        if holding['shares'] > 0:
                            viac_total_invested += holding['total_cost']
                    elif key.startswith('ING_DIBA_'):
                        if holding['shares'] > 0:
                            ing_diba_total_current += holding.get('current_value', holding['total_cost'])
                
                # Add VIAC account if there are holdings
                if viac_total_invested > 0:
                    accounts_list.append({
                        'account': 'VIAC',
                        'balance': viac_total_invested,
                        'currency': 'CHF',
                        'transaction_count': 0,
                        'last_transaction_date': None
                    })
                    totals['CHF'] += viac_total_invested
                
                # Add ING DiBa account if there are holdings
                if ing_diba_total_current > 0:
                    accounts_list.append({
                        'account': 'ING DiBa',
                        'balance': ing_diba_total_current,
                        'currency': 'EUR',
                        'transaction_count': 0,
                        'last_transaction_date': None
                    })
                    totals['EUR'] += ing_diba_total_current
        except Exception as e:
            print(f"Error adding broker accounts: {e}")
            import traceback
            traceback.print_exc()
            # Continue even if broker accounts fail

        return jsonify({
            'accounts': accounts_list,
            'totals': {k: round(v, 2) for k, v in totals.items()}
        })
    except Exception as e:
        print(f"Error getting accounts: {e}")
        import traceback
        traceback.print_exc()
        # On error, return empty accounts (user will see onboarding)
        return jsonify({'accounts': [], 'totals': {}})

@app.route('/api/broker')
@authenticate_request
@require_auth
def get_broker():
    tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
    parser = BankStatementParser()

    transactions = []
    holdings_dict = {}
    total_invested_chf = 0
    total_invested_eur = 0
    total_current_value_eur = 0

    # Try to read from uploaded documents in database first
    try:
        broker_docs = wealth_db.list_file_attachments(tenant_id, file_types=['broker_viac_pdf', 'broker_ing_diba_csv'])
        
        if broker_docs:
            encryption_service = get_encryption_service()
            import tempfile
            
            for doc in broker_docs:
                tmp_path = None
                try:
                    # Get full document with encrypted data
                    full_doc = wealth_db.get_file_attachment(tenant_id, doc['id'])
                    if not full_doc:
                        print(f"‚ö†Ô∏è Broker document {doc.get('id')} not found in database")
                        continue
                    
                    # Decrypt and save to temp file
                    encryption_metadata = full_doc.get('encryption_metadata') or {}
                    if isinstance(encryption_metadata, str):
                        try:
                            encryption_metadata = json.loads(encryption_metadata)
                        except json.JSONDecodeError as e:
                            print(f"‚ö†Ô∏è Invalid encryption metadata for broker document {doc.get('id')}: {e}")
                            continue
                    
                    nonce_b64 = encryption_metadata.get('server_encryption', {}).get('nonce', '')
                    if not nonce_b64:
                        print(f"‚ö†Ô∏è Missing nonce in encryption metadata for broker document {doc.get('id')}")
                        continue
                    
                    try:
                        nonce = base64.b64decode(nonce_b64)
                    except Exception as e:
                        print(f"‚ö†Ô∏è Invalid nonce format for broker document {doc.get('id')}: {e}")
                        continue
                    
                    # Check if server encryption was applied
                    server_encryption = encryption_metadata.get('server_encryption', {})
                    server_algorithm = server_encryption.get('algorithm', 'AES-256-GCM')
                    
                    if server_algorithm == 'none':
                        # No server encryption - file is stored as-is (shouldn't happen with current design)
                        # This means the file might be client-encrypted or plaintext
                        decrypted = full_doc['encrypted_data']
                    else:
                        # Server encryption was applied - decrypt it first
                        encrypted_data = EncryptedData(
                            ciphertext=full_doc['encrypted_data'],
                            nonce=nonce,
                            key_version=server_encryption.get('key_version', 1)
                        )
                        
                        # Create a copy of metadata for AAD verification
                        # IMPORTANT: We must remove fields that were added AFTER encryption
                        # The upload process adds nonce and key_version to server_encryption dict after encrypting
                        # The store_encrypted_file function adds checksum to file_info after encrypting
                        verification_metadata = encryption_metadata.copy()
                        
                        if 'server_encryption' in verification_metadata:
                            verification_metadata['server_encryption'] = verification_metadata['server_encryption'].copy()
                            verification_metadata['server_encryption'].pop('nonce', None)
                            verification_metadata['server_encryption'].pop('key_version', None)
                        
                        if 'file_info' in verification_metadata:
                            verification_metadata['file_info'] = verification_metadata['file_info'].copy()
                            verification_metadata['file_info'].pop('checksum', None)

                        metadata_json = json.dumps(verification_metadata, sort_keys=True).encode()
                        try:
                            decrypted = encryption_service.decrypt_data(encrypted_data, tenant_id, metadata_json)
                        except ValueError as e:
                            # Decryption failed - this is a legacy file that can't be re-read
                            # Transactions may already be in the database from initial import
                            print(f"‚ö†Ô∏è Cannot decrypt broker document {doc.get('id')} ({doc.get('original_name', 'unknown')}): {e}")
                            print(f"   This appears to be a legacy file with incompatible encryption. Data may already be loaded.")
                            print(f"   To update holdings, delete and re-upload this file.")
                            continue
                    
                    # Decrypt client layer if present (double encryption - legacy support)
                    client_metadata = encryption_metadata.get('client_encryption')
                    
                    # Check if client-side encryption was used (algorithm != 'none')
                    is_client_encrypted = (
                        client_metadata 
                        and client_metadata.get('algorithm') != 'none'
                        and client_metadata.get('nonce')
                    )
                    
                    if is_client_encrypted and g.session_token:
                        try:
                            decrypted = encryption_service.decrypt_client_layer(
                                decrypted, 
                                g.session_token, 
                                client_metadata
                            )
                        except Exception as e:
                            print(f"‚ö†Ô∏è Cannot decrypt client layer for broker document {doc.get('id')}: {e}")
                            continue
                    
                    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(doc['original_name'])[1]) as tmp:
                        tmp.write(decrypted)
                        tmp_path = tmp.name
                    
                    # Parse based on document type
                    file_type = doc.get('file_type') or full_doc.get('file_type')
                    if file_type == 'broker_viac_pdf':
                        file_transactions = parser.parse_viac(tmp_path)
                    elif file_type == 'broker_ing_diba_csv':
                        file_holdings = parser.parse_ing_diba(tmp_path)
                        # Convert holdings to transactions format for consistency
                        for holding in file_holdings:
                            holdings_dict[f"ING_DIBA_{holding['isin']}"] = holding
                        if tmp_path:
                            os.unlink(tmp_path)
                        continue
                    else:
                        if tmp_path:
                            os.unlink(tmp_path)
                        continue
                    
                    transactions.extend(file_transactions)
                    
                    # Aggregate VIAC holdings by ISIN
                    for t in file_transactions:
                        key = f"VIAC_{t['isin']}"
                        if key not in holdings_dict:
                            holdings_dict[key] = {
                                'isin': t['isin'],
                                'security': t['security'],
                                'shares': 0,
                                'total_cost': 0,
                                'current_value': 0,
                                'average_cost': 0,
                                'currency': 'CHF',
                                'account': 'VIAC',
                                'transaction_count': 0
                            }
                        holdings_dict[key]['shares'] += t['shares'] if t['type'] == 'buy' else -t['shares']
                        holdings_dict[key]['total_cost'] += abs(t['amount']) if t['type'] == 'buy' else -abs(t['amount'])
                        holdings_dict[key]['transaction_count'] += 1
                    
                    if tmp_path:
                        os.unlink(tmp_path)
                        tmp_path = None
                except Exception as e:
                    print(f"‚ö†Ô∏è Error processing broker document {doc.get('id')}: {e}")
                    import traceback
                    traceback.print_exc()
                    # Clean up temp file if it exists
                    if tmp_path and os.path.exists(tmp_path):
                        try:
                            os.unlink(tmp_path)
                        except:
                            pass
                    continue
    except Exception as e:
        print(f"Error reading broker documents from database: {e}")
        import traceback
        traceback.print_exc()

    # Note: File system fallback removed - broker data now only comes from uploaded documents in database
    # This ensures that when files are deleted, broker data is properly cleared

    # Calculate totals and prepare holdings list
    holdings_list = []
    viac_total_invested = 0
    viac_total_current = 0
    ing_diba_total_invested = 0
    ing_diba_total_current = 0

    for key, holding in holdings_dict.items():
        # Skip holdings with zero or negative shares (fully sold positions)
        if holding['shares'] <= 0:
            continue
        
        # Only include holdings with positive shares in totals
        if holding['account'] == 'VIAC':
            viac_total_invested += holding['total_cost']
            # VIAC holdings use cost basis (total_cost) as current_value since we don't fetch market prices
            # Users can see current values from uploaded documents if available
            if holding.get('current_value', 0) == 0:
                holding['current_value'] = holding['total_cost']
            viac_total_current += holding.get('current_value', 0)
        elif holding['account'] == 'ING DiBa':
            ing_diba_total_invested += holding['total_cost']
            ing_diba_total_current += holding.get('current_value', holding['total_cost'])
        
        # Calculate average cost
        holding['average_cost'] = holding['total_cost'] / holding['shares']
        
        holdings_list.append(holding)

    # Prepare summary
    summary = {}
    if viac_total_invested > 0:
        summary['viac'] = {
            'total_invested': round(viac_total_invested, 2),
            'currency': 'CHF'
        }
    if ing_diba_total_invested > 0:
        summary['ing_diba'] = {
            'total_invested': round(ing_diba_total_invested, 2),
            'total_current_value': round(ing_diba_total_current, 2),
            'currency': 'EUR'
        }

    return jsonify({
        'transactions': transactions,
        'holdings': holdings_list,
        'summary': summary
    })

@app.route('/api/broker/historical-valuation')
@authenticate_request
@require_auth
def get_broker_historical_valuation():
    """
    Calculate historical portfolio valuation using only prices from uploaded broker documents.
    Uses exponential interpolation between document snapshots.
    """
    tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
    parser = BankStatementParser()
    
    # Optional ?refresh=true to force recomputation
    refresh = request.args.get('refresh', 'false').lower() in ('1', 'true', 'yes')

    try:
        # Try cached valuation first unless refresh was requested
        if not refresh:
            cached = wealth_db.get_broker_valuation_cache(tenant_id)
            if cached and cached.get("data"):
                data = cached["data"]
                if isinstance(data, dict):
                    ts_len = len(data.get('time_series', []))
                    print(f"‚úÖ Returning cached broker valuation for tenant {tenant_id}; time_series={ts_len}")
                else:
                    print(f"‚úÖ Returning cached broker valuation for tenant {tenant_id} (non-dict data)")
                return jsonify(data)

        transactions = []
        document_snapshots = []  # List of (date, total_portfolio_value) from documents
        ing_diba_purchase_date = None  # Track ING DiBa purchase date for interpolation
        ing_diba_purchase_invested = 0  # Invested capital at purchase date
        
        # Get broker documents and parse them
        try:
            broker_docs = wealth_db.list_file_attachments(tenant_id, file_types=['broker_viac_pdf', 'broker_ing_diba_csv'])
            
            if broker_docs:
                encryption_service = get_encryption_service()
                import tempfile
                
                for doc in broker_docs:
                    try:
                        full_doc = wealth_db.get_file_attachment(tenant_id, doc['id'])
                        if not full_doc:
                            continue
                        
                        encryption_metadata = full_doc.get('encryption_metadata') or {}
                        if isinstance(encryption_metadata, str):
                            encryption_metadata = json.loads(encryption_metadata)
                        
                        # Check if server encryption was applied
                        server_encryption = encryption_metadata.get('server_encryption', {})
                        server_algorithm = server_encryption.get('algorithm', 'AES-256-GCM')
                        
                        if server_algorithm == 'none':
                            # No server encryption - file is stored as-is
                            decrypted = full_doc['encrypted_data']
                        else:
                            # Server encryption was applied - decrypt it first
                            nonce_b64 = server_encryption.get('nonce', '')
                            if not nonce_b64:
                                print(f"‚ö†Ô∏è Missing nonce in encryption metadata for broker document {doc.get('id')}")
                                continue
                            
                            try:
                                nonce = base64.b64decode(nonce_b64)
                            except Exception as e:
                                print(f"‚ö†Ô∏è Invalid nonce format for broker document {doc.get('id')}: {e}")
                                continue
                            
                            encrypted_data = EncryptedData(
                                ciphertext=full_doc['encrypted_data'],
                                nonce=nonce,
                                key_version=server_encryption.get('key_version', 1)
                            )
                            
                            # Create a copy of metadata for AAD verification
                            # IMPORTANT: We must remove fields that were added AFTER encryption
                            verification_metadata = encryption_metadata.copy()
                            if 'server_encryption' in verification_metadata:
                                verification_metadata['server_encryption'] = verification_metadata['server_encryption'].copy()
                                verification_metadata['server_encryption'].pop('nonce', None)
                                verification_metadata['server_encryption'].pop('key_version', None)
                            
                            if 'file_info' in verification_metadata:
                                verification_metadata['file_info'] = verification_metadata['file_info'].copy()
                                verification_metadata['file_info'].pop('checksum', None)
                            
                            metadata_json = json.dumps(verification_metadata, sort_keys=True).encode()
                            try:
                                decrypted = encryption_service.decrypt_data(encrypted_data, tenant_id, metadata_json)
                            except ValueError as e:
                                # Decryption failed - legacy file, skip but data may already be in DB
                                print(f"‚ö†Ô∏è Cannot decrypt broker document {doc.get('id')} ({doc.get('original_name', 'unknown')}): {e}")
                                print(f"   Skipping legacy file - data may already be loaded from previous import.")
                                continue
                        
                        # Decrypt client layer if present (double encryption - legacy support)
                        client_metadata = encryption_metadata.get('client_encryption')
                        
                        # Check if client-side encryption was used (algorithm != 'none')
                        is_client_encrypted = (
                            client_metadata 
                            and client_metadata.get('algorithm') != 'none'
                            and client_metadata.get('nonce')
                        )
                        
                        if is_client_encrypted and g.session_token:
                            try:
                                decrypted = encryption_service.decrypt_client_layer(
                                    decrypted, 
                                    g.session_token, 
                                    client_metadata
                                )
                            except Exception as e:
                                print(f"‚ö†Ô∏è Cannot decrypt client layer for broker document {doc.get('id')}: {e}")
                                continue
                        
                        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(doc['original_name'])[1]) as tmp:
                            tmp.write(decrypted)
                            tmp_path = tmp.name
                        
                        file_type = doc.get('file_type') or full_doc.get('file_type')
                        if file_type == 'broker_viac_pdf':
                            file_transactions = parser.parse_viac(tmp_path)
                            transactions.extend(file_transactions)
                        elif file_type == 'broker_ing_diba_csv':
                            # Parse ING DiBa CSV and extract snapshot
                            file_holdings = parser.parse_ing_diba(tmp_path)
                            if file_holdings:
                                # Get snapshot date from first holding (all have same date)
                                snapshot_date_str = file_holdings[0].get('date')
                                # Get purchase date and total cost from holdings
                                purchase_date_str = file_holdings[0].get('purchase_date')
                                if purchase_date_str and not ing_diba_purchase_date:
                                    ing_diba_purchase_date = purchase_date_str
                                
                                if snapshot_date_str:
                                    # Calculate total portfolio value from this document
                                    total_value_chf = 0
                                    total_cost_chf = 0
                                    for holding in file_holdings:
                                        current_val = holding.get('current_value', 0) or 0
                                        cost_val = holding.get('total_cost', 0) or 0
                                        currency = holding.get('currency', 'EUR')
                                        # Convert to CHF
                                        if currency == 'EUR':
                                            total_value_chf += current_val * 0.95
                                            total_cost_chf += cost_val * 0.95
                                        elif currency == 'CHF':
                                            total_value_chf += current_val
                                            total_cost_chf += cost_val
                                        else:
                                            total_value_chf += current_val  # Default
                                            total_cost_chf += cost_val
                                    
                                    if total_value_chf > 0:
                                        document_snapshots.append((snapshot_date_str, total_value_chf))
                                        print(f"üìÑ Document snapshot: {snapshot_date_str} = {total_value_chf:.2f} CHF")
                                    
                                    # Track purchase invested if we have purchase date
                                    if purchase_date_str and total_cost_chf > 0:
                                        ing_diba_purchase_invested = total_cost_chf
                        
                        os.unlink(tmp_path)
                    except Exception as e:
                        print(f"Error processing broker document {doc.get('id')}: {e}")
                        continue
        except Exception as e:
            print(f"Error reading broker documents: {e}")
        
        # Note: File system fallback removed - historical valuation now only uses uploaded documents in database
        # This ensures that when files are deleted, historical data is properly cleared
        
        # If no data, return empty result
        if not transactions and not document_snapshots:
            empty_payload = {
                'time_series': [],
                'error': None
            }
            wealth_db.save_broker_valuation_cache(tenant_id, empty_payload)
            return jsonify(empty_payload)
        
        # Sort document snapshots by date
        document_snapshots.sort(key=lambda x: x[0])
        print(f"üìä Found {len(document_snapshots)} document snapshots")
        
        # Sort transactions by date
        sorted_transactions = sorted(transactions, key=lambda x: x['date'])
        
        # Build invested capital timeline from transactions
        invested_by_date = {}  # date_str -> cumulative_invested
        cumulative_invested = 0
        
        # Get all dates we need to calculate
        dates_to_calculate = set()
        
        # Add transaction dates
        for transaction in sorted_transactions:
            trans_date = transaction['date'].split('T')[0]
            dates_to_calculate.add(trans_date)
            cumulative_invested += abs(transaction.get('amount', 0))
            invested_by_date[trans_date] = cumulative_invested
        
        # Add document snapshot dates
        for snapshot_date_str, _ in document_snapshots:
            dates_to_calculate.add(snapshot_date_str.split('T')[0])
        
        # Add today
        today_str = datetime.now().strftime('%Y-%m-%d')
        dates_to_calculate.add(today_str)
        
        # Generate monthly data points between first and last date for smooth interpolation
        if dates_to_calculate:
            sorted_temp_dates = sorted(dates_to_calculate)
            first_date = datetime.fromisoformat(sorted_temp_dates[0])
            last_date = datetime.fromisoformat(sorted_temp_dates[-1])
            
            # Add first day of each month between first and last date
            # Start from the month containing the first date
            current_month = datetime(first_date.year, first_date.month, 1)
            last_month = datetime(last_date.year, last_date.month, 1)
            
            while current_month <= last_month:
                month_start = current_month.strftime('%Y-%m-%d')
                dates_to_calculate.add(month_start)
                # Move to next month
                if current_month.month == 12:
                    current_month = datetime(current_month.year + 1, 1, 1)
                else:
                    current_month = datetime(current_month.year, current_month.month + 1, 1)
            
            print(f"üìÖ Generated monthly data points: {len(dates_to_calculate)} total dates")
        
        # Sort all dates
        sorted_dates = sorted(dates_to_calculate)
        
        if not sorted_dates:
            sorted_dates = [today_str]
        
        # Build time series with exponential interpolation between snapshots
        time_series = []
        
        # Create snapshot lookup: date_str -> portfolio_value
        snapshot_values = {}
        for snapshot_date_str, value in document_snapshots:
            date_key = snapshot_date_str.split('T')[0]
            snapshot_values[date_key] = value
        
        # Find purchase date for interpolation (use ING DiBa purchase date if available, otherwise first transaction)
        purchase_date_for_interpolation = None
        purchase_invested = 0
        
        print(f"üîç ING DiBa purchase date: {ing_diba_purchase_date}, invested: {ing_diba_purchase_invested}")
        print(f"üîç Document snapshots: {len(document_snapshots)}, Transactions: {len(sorted_transactions)}")
        
        # Prefer ING DiBa purchase date if we have it
        if ing_diba_purchase_date and ing_diba_purchase_invested > 0:
            purchase_date_for_interpolation = ing_diba_purchase_date.split('T')[0] if 'T' in ing_diba_purchase_date else ing_diba_purchase_date
            purchase_invested = ing_diba_purchase_invested
            print(f"‚úÖ Using ING DiBa purchase date: {purchase_date_for_interpolation}, invested: {purchase_invested:.2f}")
        elif document_snapshots and sorted_transactions:
            # Use first transaction date as purchase date
            first_trans_date = sorted_transactions[0]['date'].split('T')[0]
            purchase_date_for_interpolation = first_trans_date
            purchase_invested = invested_by_date.get(first_trans_date, 0)
            print(f"‚úÖ Using first transaction date: {purchase_date_for_interpolation}, invested: {purchase_invested:.2f}")
        
        # If we have document snapshots but no purchase snapshot, add one at the purchase date
        # This allows interpolation from purchase to first document snapshot
        if purchase_date_for_interpolation and document_snapshots:
            first_snapshot_date_str = document_snapshots[0][0].split('T')[0]
            purchase_date_key = purchase_date_for_interpolation.split('T')[0] if 'T' in purchase_date_for_interpolation else purchase_date_for_interpolation
            
            print(f"üîç Comparing purchase date {purchase_date_key} with first snapshot {first_snapshot_date_str}")
            
            # Only add purchase snapshot if it's before the first document snapshot
            if purchase_date_key < first_snapshot_date_str and purchase_invested > 0:
                snapshot_values[purchase_date_key] = purchase_invested
                # Also add to dates_to_calculate if not already there
                dates_to_calculate.add(purchase_date_key)
                print(f"üìÖ Added purchase snapshot at {purchase_date_key}: {purchase_invested:.2f} CHF (for interpolation)")
            else:
                print(f"‚ö†Ô∏è Not adding purchase snapshot: purchase_date={purchase_date_key}, first_snapshot={first_snapshot_date_str}, invested={purchase_invested}")
        
        # Re-sort dates after adding purchase date
        sorted_dates = sorted(dates_to_calculate)
        
        # Find first and last snapshot dates
        snapshot_dates = sorted(snapshot_values.keys())
        first_snapshot_date = snapshot_dates[0] if snapshot_dates else None
        last_snapshot_date = snapshot_dates[-1] if snapshot_dates else None
        
        # Find first investment date - filter out dates before first investment
        first_investment_date = None
        if sorted_transactions:
            first_investment_date = sorted_transactions[0]['date'].split('T')[0]
        elif ing_diba_purchase_date:
            first_investment_date = ing_diba_purchase_date.split('T')[0] if 'T' in ing_diba_purchase_date else ing_diba_purchase_date
        
        # Filter dates to only include from first investment onwards
        if first_investment_date:
            sorted_dates = [d for d in sorted_dates if d >= first_investment_date]
            print(f"üìÖ Filtered dates: starting from first investment {first_investment_date}")
        
        for date_str in sorted_dates:
            date_obj = datetime.fromisoformat(date_str)
            
            # Calculate invested capital at this date
            invested = invested_by_date.get(date_str, 0)
            if not invested and sorted_dates.index(date_str) > 0:
                # Forward-fill invested capital
                prev_dates = [d for d in sorted_dates if d < date_str]
                if prev_dates:
                    invested = invested_by_date.get(prev_dates[-1], 0)
            
            # Skip if no investment yet (shouldn't happen after filtering, but safety check)
            if invested == 0 and date_str != first_investment_date:
                continue
            
            point = {
                'date': date_str,
                'invested': round(invested, 2)
            }
            
            # Calculate portfolio value - always set it
            portfolio_value = None
            
            # If this is a snapshot date, use the exact value
            if date_str in snapshot_values:
                portfolio_value = snapshot_values[date_str]
                print(f"üì∏ Snapshot at {date_str}: {portfolio_value:.2f} CHF")
            elif snapshot_dates:
                # Interpolate between snapshots using exponential growth
                if date_str < first_snapshot_date:
                    # Before first snapshot: use invested capital
                    portfolio_value = invested
                elif date_str > last_snapshot_date:
                    # After last snapshot: keep flat at last snapshot value
                    portfolio_value = snapshot_values[last_snapshot_date]
                else:
                    # Between snapshots: exponential interpolation
                    # Find the two snapshots this date falls between
                    prev_snapshot_date = None
                    next_snapshot_date = None
                    prev_value = None
                    next_value = None
                    
                    for i, snap_date in enumerate(snapshot_dates):
                        if snap_date <= date_str:
                            prev_snapshot_date = snap_date
                            prev_value = snapshot_values[snap_date]
                        if snap_date > date_str and next_snapshot_date is None:
                            next_snapshot_date = snap_date
                            next_value = snapshot_values[snap_date]
                            break
                    
                    if prev_snapshot_date and next_snapshot_date and prev_value > 0:
                        # Calculate exponential growth rate
                        days_between = (datetime.fromisoformat(next_snapshot_date) - datetime.fromisoformat(prev_snapshot_date)).days
                        days_elapsed = (date_obj - datetime.fromisoformat(prev_snapshot_date)).days
                        
                        if days_between > 0:
                            growth_ratio = next_value / prev_value
                            t = days_elapsed / days_between  # t in [0, 1]
                            portfolio_value = prev_value * (growth_ratio ** t)
                        else:
                            portfolio_value = prev_value
                    elif prev_snapshot_date:
                        # Only have previous snapshot, keep flat
                        portfolio_value = prev_value
                    else:
                        # Fallback to invested
                        portfolio_value = invested
            else:
                # No snapshots at all: use invested capital
                portfolio_value = invested
            
            # Always set portfolio value (even if it equals invested)
            if portfolio_value is None:
                portfolio_value = invested
            
            point['value'] = round(portfolio_value, 2)
            time_series.append(point)
        
        # Debug: Count points with value vs invested
        points_with_value = sum(1 for p in time_series if p.get('value') is not None)
        points_with_different_value = sum(1 for p in time_series if p.get('value') is not None and abs(p.get('value', 0) - p.get('invested', 0)) > 0.01)
        print(f"üìä Generated {len(time_series)} data points: {points_with_value} have value, {points_with_different_value} differ from invested")
        
        payload = {
            'time_series': time_series,
            'error': None
        }

        # Store in cache
        wealth_db.save_broker_valuation_cache(tenant_id, payload)
        print(f"üíæ Saved broker valuation cache: {len(time_series)} data points")

        return jsonify(payload)
        
    except Exception as e:
        import traceback
        print(f"Error calculating historical valuation: {e}")    
        print(traceback.format_exc())

        # On error, fall back to cached data if available
        cached = wealth_db.get_broker_valuation_cache(tenant_id)
        if cached and cached.get("data"):
            print(f"‚ö†Ô∏è Error during valuation; returning last cached data for tenant {tenant_id}")
            return jsonify(cached["data"])

        return jsonify({
            'time_series': [],
            'error': str(e)
        })


@app.route('/api/projection')
@authenticate_request
@require_auth
def get_projection():
    """Get wealth projection data based on current net worth and savings history"""
    tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'

    try:
        # Get accounts for current net worth
        accounts = wealth_db.get_accounts(tenant_id)
        current_net_worth = sum(account['balance'] for account in accounts)

        # Get summary data for savings calculation
        summary_data = wealth_db.get_summary_data(tenant_id, months=6)

        if not summary_data:
            return jsonify({
                'currentNetWorth': current_net_worth,
                'averageMonthlySavings': 0,
                'averageSavingsRate': 0
            })

        # Calculate average monthly savings from recent months
        total_savings = sum(month['savings'] for month in summary_data)
        total_income = sum(month['income'] for month in summary_data)

        average_monthly_savings = total_savings / len(summary_data) if summary_data else 0
        average_savings_rate = (total_savings / total_income * 100) if total_income > 0 else 0

        return jsonify({
            'currentNetWorth': current_net_worth,
            'averageMonthlySavings': average_monthly_savings,
            'averageSavingsRate': average_savings_rate
        })

    except Exception as e:
        print(f"Error getting projection data: {e}")
        # Return minimal valid data to prevent frontend crash
        return jsonify({
            'currentNetWorth': 0,
            'averageMonthlySavings': 0,
            'averageSavingsRate': 0
        })
    
@app.route('/api/loans')
@authenticate_request
@require_auth
def get_loans():
    """
    Return loan data. Demo PDF parsing is disabled unless ENABLE_DEMO_LOANS=true.
    """
    tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'

    # Placeholder for future database-backed loans. Currently returns empty sets.
    loans = []
    total_loan_balance = 0
    total_monthly_payment = 0

    use_demo_loans = os.environ.get('ENABLE_DEMO_LOANS', '').lower() in ('1', 'true', 'yes')

    if use_demo_loans:
        parser = BankStatementParser()
        base_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'credits')
        kfw_folder = os.path.join(base_path, 'kfw')

        if os.path.exists(kfw_folder):
            kfw_files = glob.glob(os.path.join(kfw_folder, '*.pdf')) + glob.glob(os.path.join(kfw_folder, '*.PDF'))
            for kfw_file in kfw_files:
                file_loans = parser.parse_kfw(kfw_file)
                loans.extend(file_loans)

                for loan in file_loans:
                    total_loan_balance += loan['current_balance']
                    total_monthly_payment += loan['monthly_payment']

        loans.sort(key=lambda x: x['program'])

    return jsonify({
        'loans': loans,
        'summary': {
            'total_balance': round(total_loan_balance, 2),
            'total_monthly_payment': round(total_monthly_payment, 2),
            'loan_count': len(loans)
        }
    })


@app.route('/api/categories', methods=['GET'])
@authenticate_request
@require_auth
def get_categories():
    """Get all available categories including custom ones"""
    try:
        tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
        
        # Load default categories from JSON files
        spending_categories = _load_categories('categories_spending.json')
        income_categories = _load_categories('categories_income.json')
        
        # Get tenant-specific custom categories from database
        db_categories = wealth_db.get_categories(tenant_id)
        
        # Combine default and custom categories (return just names)
        all_categories = {
            'income': list(income_categories.keys()) + [c['category_name'] for c in db_categories.get('income', [])],
            'expense': list(spending_categories.keys()) + [c['category_name'] for c in db_categories.get('expense', [])]
        }
        
        return jsonify(all_categories)
    except Exception as e:
        print(f"Error getting categories: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'income': [], 'expense': []})


@app.route('/api/categories', methods=['POST'])
@authenticate_request
@require_auth
def create_custom_category():
    """Create a new custom category"""
    try:
        tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
        data = request.get_json()
        category_name = data.get('name', '').strip()
        category_type = data.get('type', 'expense')
        
        print(f"Creating custom category for tenant {tenant_id}: {category_name} ({category_type})")
        
        if not category_name:
            return jsonify({'error': 'Category name is required'}), 400
        
        if category_type not in ['income', 'expense']:
            return jsonify({'error': 'Category type must be income or expense'}), 400
        
        # Check if category already exists in default categories
        if category_type == 'expense':
            default_categories = _load_categories('categories_spending.json')
        else:
            default_categories = _load_categories('categories_income.json')
        
        if category_name in default_categories:
            return jsonify({'error': 'Category already exists as a default category'}), 400
        
        # Create category in database
        try:
            wealth_db.create_custom_category(tenant_id, category_name, category_type)
            print(f"‚úì Custom category created successfully")
            return jsonify({'success': True, 'message': 'Custom category created successfully'})
        except ValueError as e:
            # Duplicate category
            print(f"Category already exists: {e}")
            return jsonify({'error': str(e)}), 400
        
    except Exception as e:
        print(f"Error creating custom category: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to create custom category'}), 500

@app.route('/api/update-category', methods=['POST'])
@authenticate_request
@require_auth
def update_category():
    """Update the category of a specific transaction"""
    try:
        tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
        data = request.get_json()
        transaction = data.get('transaction') or {}
        new_category = data.get('newCategory')

        print(f"=== UPDATE CATEGORY ===")
        print(f"Tenant: {tenant_id}, New category: {new_category}")
        print(f"Transaction data: {transaction}")

        if not transaction:
            print("ERROR: No transaction data provided")
            return jsonify({'error': 'Missing transaction data'}), 400
            
        if not new_category:
            print("ERROR: No new category provided")
            return jsonify({'error': 'Missing newCategory'}), 400

        # Get the transaction hash (should be included in transaction object)
        transaction_hash = transaction.get('transaction_hash')
        
        if not transaction_hash:
            print(f"ERROR: Missing transaction_hash in transaction object. Keys available: {list(transaction.keys())}")
            return jsonify({'error': 'Missing transaction_hash - please refresh the page'}), 400

        print(f"Updating transaction {transaction_hash} to category: {new_category}")

        # Update the category in the database
        wealth_db.create_category_override(
            tenant_id=tenant_id,
            transaction_hash=transaction_hash,
            override_category=new_category,
            reason='Manual user override'
        )

        print(f"‚úì Category updated successfully")
        return jsonify({'success': True, 'message': 'Category updated successfully'})

    except Exception as e:
        print(f"Error updating category: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Failed to update category: {str(e)}'}), 500

@app.route('/api/upload-statement', methods=['POST'])
@authenticate_request
@require_auth
def upload_statement():
    """
    Upload and store encrypted bank statements

    Implements the secure file upload process:
    1. Receive client-encrypted file
    2. Add server-side encryption layer
    3. Store encrypted blob with metadata
    4. Return secure reference for future access
    """
    try:
        if 'encryptedFile' not in request.files or 'encryptionMetadata' not in request.form:
            return jsonify({'error': 'Missing encrypted file or metadata'}), 400

        encrypted_file = request.files['encryptedFile']
        metadata_str = request.form['encryptionMetadata']

        # Parse encryption metadata
        try:
            metadata = json.loads(metadata_str)
        except json.JSONDecodeError:
            return jsonify({'error': 'Invalid encryption metadata'}), 400

        document_type = (
            request.form.get('documentType')
            or metadata.get('fileMetadata', {}).get('documentType')
            or 'bank_statement_dkb'
        )

        document_metadata_raw = request.form.get('documentMetadata')
        document_metadata = None
        if document_metadata_raw:
            try:
                document_metadata = json.loads(document_metadata_raw)
            except json.JSONDecodeError:
                document_metadata = {'raw': document_metadata_raw}

        metadata.setdefault('fileMetadata', {})
        metadata['fileMetadata']['documentType'] = document_type

        # Read the client-encrypted file data
        client_ciphertext = encrypted_file.read()

        # Get encryption service for server-side encryption
        encryption_service = get_encryption_service()

        # Create server-side encryption metadata
        server_metadata = {
            'client_encryption': metadata,
            'server_encryption': {
                'algorithm': 'AES-256-GCM',
                'encrypted_at': datetime.now(timezone.utc).isoformat()
            },
            'file_info': {
                'original_name': metadata['originalName'],
                'original_size': metadata['originalSize'],
                'original_type': metadata['originalType'],
                'uploaded_at': datetime.now(timezone.utc).isoformat(),
                'document_type': document_type
            },
            'document_metadata': document_metadata
        }

        # Encrypt with server-side encryption
        tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
        associated_data = json.dumps(server_metadata, sort_keys=True).encode()
        server_encrypted_data = encryption_service.encrypt_data(
            client_ciphertext,
            tenant_id,
            associated_data
        )

        # Store encrypted file in database
        file_record = wealth_db.store_encrypted_file(
            tenant_id=tenant_id,
            encrypted_data=server_encrypted_data['encrypted_data'],
            metadata=server_metadata,
            file_type=document_type,
            uploaded_by=_get_authenticated_user_id()
        )

        return jsonify({
            'success': True,
            'file_id': file_record['id'],
            'message': 'File uploaded and encrypted successfully'
        })
    except Exception as e:
        print(f"Error uploading statement: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to upload statement'}), 500


@app.route('/api/documents', methods=['GET'])
@authenticate_request
@require_auth
def get_documents():
    """Return existing uploaded documents for the authenticated tenant."""
    try:
        tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
        attachments = wealth_db.list_file_attachments(tenant_id)
        documents = [_serialize_document_record(record) for record in attachments]

        return jsonify({
            'documentTypes': DOCUMENT_TYPES,
            'documents': documents
        })
    except Exception as e:
        print(f"Error fetching documents: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to load documents'}), 500


@app.route('/api/documents/upload', methods=['POST'])
@authenticate_request
@require_auth
def upload_document():
    """Upload a new encrypted document for a specific document type."""
    try:
        if 'encryptedFile' not in request.files or 'encryptionMetadata' not in request.form:
            return jsonify({'error': 'Missing encrypted file or metadata'}), 400

        document_type = request.form.get('documentType')
        if not document_type:
            return jsonify({'error': 'documentType is required'}), 400

        document_type = document_type.strip()
        print(f"üìÑ Document upload - documentType received: {document_type}")
        document_config = DOCUMENT_TYPE_LOOKUP.get(document_type)
        if not document_config:
            return jsonify({'error': f'Unknown documentType: {document_type}'}), 400
        print(f"‚úì Document config found: {document_config['label']}")

        encrypted_file = request.files['encryptedFile']
        metadata_raw = request.form['encryptionMetadata']
        try:
            metadata = json.loads(metadata_raw)
        except json.JSONDecodeError:
            return jsonify({'error': 'Invalid encryption metadata'}), 400

        required_keys = ['originalName', 'originalSize', 'originalType']
        if not all(key in metadata for key in required_keys):
            return jsonify({'error': 'Invalid file metadata provided'}), 400

        original_name = metadata.get('originalName') or ''
        extension = os.path.splitext(original_name)[1].lower()
        allowed_extensions = document_config.get('extensions') or []
        if allowed_extensions and extension not in allowed_extensions:
            return jsonify({
                'error': f"Expected file with extension {', '.join(allowed_extensions)} for {document_config['label']}"
            }), 400

        document_metadata_raw = request.form.get('documentMetadata')
        document_metadata = None
        if document_metadata_raw:
            try:
                document_metadata = json.loads(document_metadata_raw)
            except json.JSONDecodeError:
                document_metadata = {'raw': document_metadata_raw}

        metadata.setdefault('fileMetadata', {})
        metadata['fileMetadata']['documentType'] = document_type

        file_data = encrypted_file.read()
        tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
        uploaded_by = _get_authenticated_user_id()
        encryption_service = get_encryption_service()
        now_iso = datetime.now(timezone.utc).isoformat()

        # Check if client already encrypted the file
        client_algorithm = metadata.get('algorithm', 'none')
        is_client_encrypted = client_algorithm != 'none' and client_algorithm is not None
        
        if is_client_encrypted:
            # Client encrypted the file - this shouldn't happen with current design
            # but we handle it gracefully by storing as-is without double encryption
            print(f"‚ö†Ô∏è Warning: Received client-encrypted file for {document_type}. Client-side encryption should be disabled.")
            # Store the client-encrypted data directly without server encryption
            server_encrypted_data = EncryptedData(
                ciphertext=file_data,
                nonce=b'',  # Empty nonce since we're not encrypting
                key_version=None
            )
            server_metadata = {
                'client_encryption': metadata,
                'document_type': document_type,
                'server_encryption': {
                    'algorithm': 'none',  # No server encryption applied
                    'encrypted_at': now_iso,
                    'key_version': None,
                    'nonce': None
                },
                'file_info': {
                    'original_name': metadata['originalName'],
                    'original_size': metadata['originalSize'],
                    'original_type': metadata['originalType'],
                    'uploaded_at': now_iso,
                    'document_type': document_type
                },
                'document_metadata': document_metadata
            }
        else:
            # File is plaintext - encrypt it on the server
            server_metadata = {
                'client_encryption': metadata,
                'document_type': document_type,
                'server_encryption': {
                    'algorithm': 'AES-256-GCM',
                    'encrypted_at': now_iso
                },
                'file_info': {
                    'original_name': metadata['originalName'],
                    'original_size': metadata['originalSize'],
                    'original_type': metadata['originalType'],
                    'uploaded_at': now_iso,
                    'document_type': document_type
                },
                'document_metadata': document_metadata
            }

            associated_data = json.dumps(server_metadata, sort_keys=True).encode()
            server_encrypted_data = encryption_service.encrypt_data(
                file_data,
                tenant_id,
                associated_data
            )

            # Include server encryption metadata (nonce/key version) for future decryption
            server_metadata['server_encryption'].update({
                'key_version': server_encrypted_data.key_version,
                'nonce': base64.b64encode(server_encrypted_data.nonce).decode('ascii')
            })

        file_record = wealth_db.store_encrypted_file(
            tenant_id=tenant_id,
            encrypted_data=server_encrypted_data.ciphertext,
            metadata=server_metadata,
            file_type=document_type,
            uploaded_by=uploaded_by
        )

        document_payload = _serialize_document_record(file_record)

        return jsonify({
            'success': True,
            'document': document_payload
        }), 201
    except Exception as e:
        print(f"Error uploading document: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to upload document'}), 500


@app.route('/api/documents/<int:document_id>', methods=['DELETE'])
@authenticate_request
@require_auth
def delete_document(document_id: int):
    """Delete a previously uploaded document."""
    try:
        tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
        deleted_info = wealth_db.delete_file_attachment(tenant_id, document_id)
        if not deleted_info:
            return jsonify({'error': 'Document not found'}), 404

        # Transactions are automatically deleted via CASCADE foreign key constraint
        print(f"‚úÖ Document {document_id} deleted. Associated transactions removed automatically via CASCADE.")

        return jsonify({'success': True, 'deletedId': document_id})
    except Exception as e:
        print(f"Error deleting document {document_id}: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to delete document'}), 500


@app.route('/api/documents/by-type/<string:document_type>', methods=['DELETE'])
@authenticate_request
@require_auth
def delete_documents_by_type(document_type: str):
    """Delete all documents of a specific type for the authenticated tenant."""
    try:
        normalized_type = (document_type or '').strip()
        if not normalized_type:
            return jsonify({'error': 'documentType is required'}), 400

        if normalized_type not in DOCUMENT_TYPE_LOOKUP:
            return jsonify({'error': f'Unknown documentType: {normalized_type}'}), 400

        tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
        deleted_records = wealth_db.delete_file_attachments_by_type(tenant_id, normalized_type)
        deleted_count = len(deleted_records)

        # Transactions are automatically deleted via CASCADE foreign key constraint
        print(f"‚úÖ Deleted {deleted_count} document(s) of type {normalized_type}. Associated transactions removed automatically via CASCADE.")

        return jsonify({
            'success': True,
            'deletedCount': deleted_count
        })
    except Exception as e:
        print(f"Error deleting documents of type {document_type}: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to delete documents'}), 500


@app.route('/api/wipe-data', methods=['POST'])
@authenticate_request
@require_auth
def wipe_tenant_data():
    """Allow an authenticated user to wipe their tenant's data."""
    try:
        tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
        user_id = _get_authenticated_user_id()
        payload = request.get_json(silent=True) or {}
        keep_custom_categories = bool(payload.get('keepCustomCategories', True))

        print(f"üßπ Wiping data for tenant={tenant_id}, keep_custom_categories={keep_custom_categories}")
        deletion_summary = wealth_db.wipe_tenant_data(
            tenant_id=tenant_id,
            keep_custom_categories=keep_custom_categories
        )

        if user_id:
            try:
                wealth_db.log_audit_event(
                    tenant_id=tenant_id,
                    user_id=user_id,
                    action='wipe_data',
                    resource_type='tenant',
                    resource_id=None,
                    key_version=None,
                    success=True
                )
            except Exception as audit_error:
                print(f"Failed to log wipe_data audit event: {audit_error}")

        return jsonify({
            'success': True,
            'keepCustomCategories': keep_custom_categories,
            'summary': deletion_summary
        })
    except Exception as e:
        print(f"Error wiping tenant data: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to wipe tenant data'}), 500


# In-memory progress tracking (keyed by upload_id)
upload_progress = {}

@app.route('/api/upload-csv', methods=['POST'])
@authenticate_request
@require_auth
def upload_csv():
    """
    Upload and parse CSV bank statements (YUH or DKB format)
    """
    try:
        tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
        
        if 'file' not in request.files:
            return jsonify({'error': 'No file provided'}), 400
        
        file = request.files['file']
        bank_type = request.form.get('bankType', 'auto')  # 'yuh', 'dkb', or 'auto'
        upload_id = request.form.get('uploadId')  # Optional upload ID for progress tracking
        document_id_str = request.form.get('documentId')  # Optional document ID to link transactions
        document_id = int(document_id_str) if document_id_str else None
        
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
        
        # Initialize progress tracking
        if upload_id:
            upload_progress[upload_id] = {
                'total': 0,
                'processed': 0,
                'imported': 0,
                'skipped': 0,
                'status': 'parsing'
            }
        
        # Save file temporarily
        import tempfile
        import uuid
        temp_dir = tempfile.gettempdir()
        temp_filename = f"{uuid.uuid4()}_{file.filename}"
        temp_path = os.path.join(temp_dir, temp_filename)
        file.save(temp_path)
        
        try:
            parser = BankStatementParser()
            
            # Parse based on bank type or auto-detect
            print(f"Parsing CSV file: {file.filename}, bank_type: {bank_type}")
            transactions = []
            min_transaction_date = None
            max_transaction_date = None
            detected_bank_type = bank_type  # Track what was actually detected
            
            if bank_type == 'yuh':
                transactions = parser.parse_yuh(temp_path)
                detected_bank_type = 'yuh'
                print(f"Parsed as YUH, found {len(transactions)} transactions")
            elif bank_type == 'dkb':
                transactions = parser.parse_dkb(temp_path)
                detected_bank_type = 'dkb'
                print(f"Parsed as DKB, found {len(transactions)} transactions")
            else:
                # Auto-detect: Try DKB first if filename suggests DKB (contains "Umsatzliste" or "Girokonto" or "Tagesgeld")
                filename_lower = file.filename.lower()
                if 'umsatzliste' in filename_lower or 'girokonto' in filename_lower or 'tagesgeld' in filename_lower or 'dkb' in filename_lower:
                    print(f"Auto-detecting as DKB based on filename: {file.filename}")
                    transactions = parser.parse_dkb(temp_path)
                    detected_bank_type = 'dkb'
                    print(f"Auto-detected as DKB, parsed {len(transactions)} transactions")
                elif 'yuh' in filename_lower or 'aktivit' in filename_lower:
                    print(f"Auto-detecting as YUH based on filename: {file.filename}")
                    transactions = parser.parse_yuh(temp_path)
                    detected_bank_type = 'yuh'
                    print(f"Auto-detected as YUH, parsed {len(transactions)} transactions")
                else:
                    # Try YUH first, then DKB
                    try:
                        transactions = parser.parse_yuh(temp_path)
                        detected_bank_type = 'yuh'
                        print(f"Auto-detected as YUH, parsed {len(transactions)} transactions")
                    except Exception as e:
                        print(f"YUH parsing failed: {e}, trying DKB...")
                        transactions = parser.parse_dkb(temp_path)
                        detected_bank_type = 'dkb'
                        print(f"Auto-detected as DKB, parsed {len(transactions)} transactions")
            
            print(f"Total transactions parsed: {len(transactions)}")
            
            if len(transactions) == 0:
                if upload_id:
                    upload_progress.pop(upload_id, None)
                return jsonify({
                    'success': False,
                    'error': 'No transactions found in CSV file. Please check the file format.',
                    'imported': 0,
                    'skipped': 0
                }), 400
            
            # Update progress: parsing complete, starting processing
            if upload_id:
                upload_progress[upload_id] = {
                    'total': len(transactions),
                    'processed': 0,
                    'imported': 0,
                    'skipped': 0,
                    'status': 'processing'
                }
            
            print(f"Starting to import {len(transactions)} transactions into database...")
            # Create accounts and transactions in database
            created_accounts = {}
            imported_count = 0
            skipped_count = 0
            
            # Pre-calculate hashes for all transactions in this batch
            # This allows us to skip duplicate checking within the same file
            current_batch_hashes = set()
            
            for idx, trans in enumerate(transactions):
                if (idx + 1) % 100 == 0:
                    print(f"Processing transaction {idx + 1}/{len(transactions)}...")
                    # Update progress every 100 transactions
                    if upload_id and upload_id in upload_progress:
                        upload_progress[upload_id]['processed'] = idx + 1
                        upload_progress[upload_id]['imported'] = imported_count
                        upload_progress[upload_id]['skipped'] = skipped_count
                
                account_name = trans.get('account', 'Unknown')
                
                # Create account if it doesn't exist
                if account_name not in created_accounts:
                    try:
                        # Check if account already exists
                        existing_accounts = wealth_db.get_accounts(tenant_id)
                        account_id = None
                        for acc in existing_accounts:
                            if acc.get('account_name') == account_name:
                                account_id = acc.get('id')
                                break
                        
                        if not account_id:
                            # Determine account type and institution
                            if 'YUH' in account_name:
                                account_type, institution, currency = ('checking', 'YUH', 'CHF')
                            elif 'DKB' in account_name:
                                account_type, institution, currency = ('checking', 'DKB', 'EUR')
                            else:
                                account_type, institution, currency = ('checking', 'Misc', 'EUR')
                            
                            # Get balance from parser if available
                            balance = 0
                            if account_name in parser.account_balances:
                                balance = parser.account_balances[account_name].get('balance', 0)
                            
                            account = wealth_db.create_account(tenant_id, {
                                'name': account_name,
                                'type': account_type,
                                'balance': balance,
                                'currency': currency,
                                'institution': institution
                            })
                            account_id = account['id']
                        created_accounts[account_name] = account_id
                    except Exception as e:
                        print(f"Error creating account {account_name}: {e}")
                        continue
                
                # Create transaction
                account_id = created_accounts[account_name]
                try:
                    transaction_data = {
                        'date': trans['date'],
                        'amount': abs(trans['amount']),
                        'currency': trans['currency'],
                        'type': trans['type'],
                        'description': trans.get('description', ''),
                        'recipient': trans.get('recipient', ''),
                        'category': trans.get('category', 'Uncategorized')
                    }

                    result = wealth_db.create_transaction(tenant_id, account_id, transaction_data, current_batch_hashes, document_id)
                    if result:
                        imported_count += 1
                        # Add the hash of successfully imported transaction to the batch set
                        if 'transaction_hash' in result:
                            current_batch_hashes.add(result['transaction_hash'])
                    else:
                        skipped_count += 1

                    # Track min/max transaction dates
                    try:
                        tx_date = datetime.fromisoformat(trans['date'])
                        if not min_transaction_date or tx_date < min_transaction_date:
                            min_transaction_date = tx_date
                        if not max_transaction_date or tx_date > max_transaction_date:
                            max_transaction_date = tx_date
                    except Exception:
                        pass
                except Exception as e:
                    if 'duplicate' in str(e).lower() or 'unique' in str(e).lower():
                        skipped_count += 1
                    else:
                        print(f"Error creating transaction: {e}")

            print(f"Finished importing transactions. Imported: {imported_count}, Skipped: {skipped_count}")
            print(f"Tenant ID used for import: {tenant_id}")
            
            # Update final progress
            if upload_id:
                upload_progress[upload_id] = {
                    'total': len(transactions),
                    'processed': len(transactions),
                    'imported': imported_count,
                    'skipped': skipped_count,
                    'status': 'complete'
                }
            
            # Clean up temp file
            os.remove(temp_path)

            print(f"Returning success response with {imported_count} imported transactions")
            start_date_iso = min_transaction_date.date().isoformat() if min_transaction_date else None
            end_date_iso = max_transaction_date.date().isoformat() if max_transaction_date else None

            return jsonify({
                'success': True,
                'message': f'Successfully imported {imported_count} transactions',
                'imported': imported_count,
                'skipped': skipped_count,
                'accounts_created': len(created_accounts),
                'tenant_id': tenant_id,  # Include tenant_id in response for debugging
                'start_date': start_date_iso,
                'end_date': end_date_iso,
                'detected_bank_type': detected_bank_type  # Return what was actually detected
            })

        except Exception as e:
            # Clean up temp file on error
            if os.path.exists(temp_path):
                os.remove(temp_path)
            # Clean up progress tracking
            if upload_id:
                upload_progress.pop(upload_id, None)
            print(f"Error parsing CSV: {e}")
            import traceback
            traceback.print_exc()
            return jsonify({'error': f'Failed to parse CSV file: {str(e)}'}), 500

    except Exception as e:
        import traceback
        print(f"Error uploading CSV: {e}")
        traceback.print_exc()
        # Clean up progress tracking
        if upload_id:
            upload_progress.pop(upload_id, None)
        return jsonify({'error': 'Failed to upload CSV file'}), 500


@app.route('/api/upload-progress/<upload_id>', methods=['GET'])
@authenticate_request
@require_auth
def get_upload_progress(upload_id):
    """Get progress for a specific upload"""
    if upload_id in upload_progress:
        progress = upload_progress[upload_id]
        return jsonify({
            'success': True,
            'total': progress['total'],
            'processed': progress['processed'],
            'imported': progress['imported'],
            'skipped': progress['skipped'],
            'status': progress['status'],
            'progress_percent': round((progress['processed'] / progress['total'] * 100) if progress['total'] > 0 else 0)
        })
    else:
        return jsonify({
            'success': False,
            'error': 'Upload ID not found'
        }), 404


@app.route('/api/predictions/dismiss', methods=['POST'])
@authenticate_request
@require_auth
def dismiss_prediction():
    """
    Dismiss a prediction so it won't show up again
    """
    tenant_id = g.session_claims.get('tenant', 'default') if g.session_claims else 'default'
    
    try:
        data = request.get_json()
        prediction_key = data.get('prediction_key')
        recurrence_type = data.get('recurrence_type', 'monthly')
        
        if not prediction_key:
            return jsonify({'error': 'prediction_key is required'}), 400
        
        # Calculate expiry date based on recurrence type
        from datetime import timedelta
        current_date = datetime.now()
        
        if recurrence_type == 'monthly':
            # Expire after 2 months
            expires_at = current_date + timedelta(days=60)
        elif recurrence_type == 'quarterly':
            # Expire after 4 months
            expires_at = current_date + timedelta(days=120)
        elif recurrence_type == 'yearly':
            # Expire after 14 months
            expires_at = current_date + timedelta(days=420)
        else:
            # Default: 2 months
            expires_at = current_date + timedelta(days=60)
        
        # Store dismissal in database
        with wealth_db.db.get_cursor() as cursor:
            # Get tenant DB ID
            cursor.execute(
                "SELECT id FROM tenants WHERE tenant_id = %s",
                [tenant_id]
            )
            tenant_result = cursor.fetchone()
            if not tenant_result:
                return jsonify({'error': 'Tenant not found'}), 404
            
            tenant_db_id = tenant_result[0]
            
            # Insert or update dismissal
            cursor.execute("""
                INSERT INTO prediction_dismissals 
                (tenant_id, prediction_key, expires_at)
                VALUES (%s, %s, %s)
                ON CONFLICT (prediction_key) 
                DO UPDATE SET dismissed_at = CURRENT_TIMESTAMP, expires_at = EXCLUDED.expires_at
            """, [tenant_db_id, prediction_key, expires_at.date()])
        
        return jsonify({
            'success': True,
            'message': 'Prediction dismissed successfully'
        })
        
    except Exception as e:
        print(f"Error dismissing prediction: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to dismiss prediction'}), 500


@app.route('/api/download-statement/<file_id>', methods=['GET'])
@authenticate_request
@require_auth
def download_statement(file_id):
    """
    Download and decrypt a previously uploaded statement

    Implements secure file retrieval with proper decryption
    """
    try:
        storage_dir = os.path.join(os.path.dirname(__file__), 'encrypted_storage')

        # Load encrypted metadata
        metadata_path = os.path.join(storage_dir, f"{file_id}.meta.enc")
        with open(metadata_path, 'r', encoding='utf-8') as f:
            encrypted_metadata = f.read()

        # Decrypt metadata
        metadata_package = decrypt_sensitive_data(encrypted_metadata)
        server_encrypted_data = metadata_package['server_encrypted']
        server_metadata = metadata_package['server_metadata']

        # Load encrypted file
        file_path = os.path.join(storage_dir, file_id)
        with open(file_path, 'rb') as f:
            server_ciphertext = f.read()

        # Reconstruct server encryption data
        server_encrypted = EncryptedData(
            ciphertext=server_ciphertext,
            nonce=base64.b64decode(server_encrypted_data['nonce']),
            key_version=server_encrypted_data['key_version'],
            algorithm=server_encrypted_data['algorithm'],
            encrypted_at=server_encrypted_data['encrypted_at']
        )

        # Decrypt server layer
        encryption_service = get_encryption_service()
        tenant_id = server_metadata['client_encryption'].get('tenantId', 'default')
        associated_data = json.dumps(server_metadata, sort_keys=True).encode()

        client_ciphertext = encryption_service.decrypt_data(
            server_encrypted,
            tenant_id,
            associated_data
        )

        # Return the client-encrypted data (client will decrypt the final layer)
        # In a production app, you might want to implement session-based access control here
        return jsonify({
            'encryptedData': base64.b64encode(client_ciphertext).decode(),
            'metadata': server_metadata['client_encryption']
        })

    except FileNotFoundError:
        return jsonify({'error': 'File not found'}), 404
    except Exception as e:
        print(f"Error downloading statement {file_id}: {e}")
        return jsonify({'error': 'Failed to retrieve file'}), 500

@app.route('/api/essential-categories', methods=['GET'])
@authenticate_request
def get_essential_categories():
    """Get user's essential categories preferences"""
    try:
        tenant_id = g.session_claims.get('tenant')
        if not tenant_id:
            return jsonify({'error': 'Tenant ID not found'}), 400
        
        categories = wealth_db.get_essential_categories(tenant_id)
        return jsonify({'categories': categories}), 200
    except Exception as e:
        print(f"Error fetching essential categories: {e}")
        return jsonify({'error': 'Failed to fetch essential categories'}), 500

@app.route('/api/essential-categories', methods=['POST'])
@authenticate_request
def save_essential_categories():
    """Save user's essential categories preferences"""
    try:
        tenant_id = g.session_claims.get('tenant')
        if not tenant_id:
            return jsonify({'error': 'Tenant ID not found'}), 400
        
        data = request.get_json()
        categories = data.get('categories', [])
        
        if not isinstance(categories, list):
            return jsonify({'error': 'Categories must be an array'}), 400
        
        wealth_db.save_essential_categories(tenant_id, categories)
        return jsonify({'success': True, 'categories': categories}), 200
    except Exception as e:
        print(f"Error saving essential categories: {e}")
        return jsonify({'error': 'Failed to save essential categories'}), 500

if __name__ == '__main__':
    app.run(debug=True, port=5001, use_reloader=True, reloader_type='stat')
